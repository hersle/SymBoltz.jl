@article{ali-haimoudHyRecFastHighly2011,
  title = {{{HyRec}}: {{A}} Fast and Highly Accurate Primordial Hydrogen and Helium Recombination Code},
  shorttitle = {{{HyRec}}},
  author = {{Ali-Ha{\"i}moud}, Yacine and Hirata, Christopher M.},
  year = 2011,
  month = feb,
  journal = {Phys. Rev. D},
  volume = {83},
  number = {4},
  eprint = {1011.3758},
  primaryclass = {astro-ph},
  pages = {043513},
  issn = {1550-7998, 1550-2368},
  doi = {10.1103/PhysRevD.83.043513},
  url = {http://arxiv.org/abs/1011.3758},
  urldate = {2024-06-17},
  abstract = {We present a state-of-the-art primordial recombination code, HyRec, including all the physical effects that have been shown to significantly affect recombination. The computation of helium recombination includes simple analytic treatments of hydrogen continuum opacity in the He I 2 1P - 1 1S line, the He I] 2 3P - 1 1S line, and treats feedback between these lines within the on-the-spot approximation. Hydrogen recombination is computed using the effective multilevel atom method, virtually accounting for an infinite number of excited states. We account for two-photon transitions from 2s and higher levels as well as frequency diffusion in Lyman-alpha with a full radiative transfer calculation. We present a new method to evolve the radiation field simultaneously with the level populations and the free electron fraction. These computations are sped up by taking advantage of the particular sparseness pattern of the equations describing the radiative transfer. The computation time for a full recombination history is \textasciitilde 2 seconds. This makes our code well suited for inclusion in Monte Carlo Markov chains for cosmological parameter estimation from upcoming high-precision cosmic microwave background anisotropy measurements.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/PDPMNJKA/Ali-Haïmoud and Hirata - 2011 - HyRec A fast and highly accurate primordial hydro.pdf}
}

@article{amendolaCosmologyFundamentalPhysics2013,
  title = {Cosmology and Fundamental Physics with the {{Euclid}} Satellite},
  author = {Amendola, Luca and Appleby, Stephen and Bacon, David and Baker, Tessa and Baldi, Marco and Bartolo, Nicola and Blanchard, Alain and Bonvin, Camille and Borgani, Stefano and Branchini, Enzo and Burrage, Clare and Camera, Stefano and Carbone, Carmelita and Casarini, Luciano and Cropper, Mark and {deRham}, Claudia and {di Porto}, Cinzia and Ealet, Anne and Ferreira, Pedro G. and Finelli, Fabio and {Garcia-Bellido}, Juan and Giannantonio, Tommaso and Guzzo, Luigi and Heavens, Alan and Heisenberg, Lavinia and Heymans, Catherine and Hoekstra, Henk and Hollenstein, Lukas and Holmes, Rory and Horst, Ole and Jahnke, Knud and Kitching, Thomas D. and Koivisto, Tomi and Kunz, Martin and La Vacca, Giuseppe and March, Marisa and Majerotto, Elisabetta and Markovic, Katarina and Marsh, David and Marulli, Federico and Massey, Richard and Mellier, Yannick and Mota, David F. and Nunes, Nelson and Percival, Will and Pettorino, Valeria and Porciani, Cristiano and Quercellini, Claudia and Read, Justin and Rinaldi, Massimiliano and Sapone, Domenico and Scaramella, Roberto and Skordis, Constantinos and Simpson, Fergus and Taylor, Andy and Thomas, Shaun and Trotta, Roberto and Verde, Licia and Vernizzi, Filippo and Vollmer, Adrian and Wang, Yun and Weller, Jochen and Zlosnik, Tom},
  year = 2013,
  month = dec,
  journal = {Living Rev. Relativ.},
  volume = {16},
  number = {1},
  eprint = {1206.1225},
  primaryclass = {astro-ph, physics:gr-qc, physics:hep-ph},
  pages = {6},
  issn = {2367-3613, 1433-8351},
  doi = {10.12942/lrr-2013-6},
  url = {http://arxiv.org/abs/1206.1225},
  urldate = {2024-09-10},
  abstract = {Euclid is a European Space Agency medium class mission selected for launch in 2019 within the Cosmic Vision 2015-2025 programme. The main goal of Euclid is to understand the origin of the accelerated expansion of the Universe. Euclid will explore the expansion history of the Universe and the evolution of cosmic structures by measuring shapes and redshifts of galaxies as well as the distribution of clusters of galaxies over a large fraction of the sky. Although the main driver for Euclid is the nature of dark energy, Euclid science covers a vast range of topics, from cosmology to galaxy evolution to planetary research. In this review we focus on cosmology and fundamental physics, with a strong emphasis on science beyond the current standard models. We discuss five broad topics: dark energy and modified gravity, dark matter, initial conditions, basic assumptions and questions of methodology in the data analysis. This review has been planned and carried out within Euclid's Theory Working Group and is meant to provide a guide to the scientific themes that will underlie the activity of the group during the preparation of the Euclid mission.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/ZDDBKE7L/Amendola et al. - 2013 - Cosmology and fundamental physics with the Euclid .pdf}
}

@article{assassiEfficientEvaluationCosmological2017,
  title = {Efficient {{Evaluation}} of {{Cosmological Angular Statistics}}},
  author = {Assassi, Valentin and Simonovi{\'c}, Marko and Zaldarriaga, Matias},
  year = 2017,
  month = nov,
  journal = {JCAP},
  volume = {2017},
  number = {11},
  eprint = {1705.05022},
  primaryclass = {astro-ph},
  pages = {054--054},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2017/11/054},
  url = {http://arxiv.org/abs/1705.05022},
  urldate = {2024-12-10},
  abstract = {Angular statistics of cosmological observables are hard to compute. The main difficulty is due to the presence of highly-oscillatory Bessel functions which need to be integrated over. In this paper, we provide a simple and fast method to compute the angular power spectrum and bispectrum of any observable. The method is based on using an FFTlog algorithm to decompose the momentumspace statistics onto a basis of power-law functions. For each power law, the integrals over Bessel functions have a simple analytical solution. This allows us to efficiently evaluate these integrals, independently of the value of the multipole . In particular, this method significantly speeds up the evaluation of the angular bispectrum compared to existing methods. To illustrate our algorithm, we compute the galaxy, lensing and CMB temperature angular power spectrum and bispectrum.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/L5XVRJYH/Assassi et al. - 2017 - Efficient Evaluation of Cosmological Angular Stati.pdf}
}

@article{ballesterosDarkEnergyNonadiabatic2010,
  title = {Dark Energy with Non-Adiabatic Sound Speed: Initial Conditions and Detectability},
  shorttitle = {Dark Energy with Non-Adiabatic Sound Speed},
  author = {Ballesteros, Guillermo and Lesgourgues, Julien},
  year = 2010,
  month = oct,
  journal = {JCAP},
  volume = {2010},
  number = {10},
  eprint = {1004.5509},
  primaryclass = {astro-ph},
  pages = {014--014},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2010/10/014},
  url = {http://arxiv.org/abs/1004.5509},
  urldate = {2025-04-25},
  abstract = {Assuming that the universe contains a dark energy fluid with a constant linear equation of state and a constant sound speed, we study the prospects of detecting dark energy perturbations using CMB data from Planck, cross-correlated with galaxy distribution maps from a survey like LSST. We update previous estimates by carrying a full exploration of the mock data likelihood for key fiducial models. We find that it will only be possible to exclude values of the sound speed very close to zero, while Planck data alone is not powerful enough for achieving any detection, even with lensing extraction. We also discuss the issue of initial conditions for dark energy perturbations in the radiation and matter epochs, generalizing the usual adiabatic conditions to include the sound speed effect. However, for most purposes, the existence of attractor solutions renders the perturbation evolution nearly independent of these initial conditions.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/N5MHEVAA/Ballesteros and Lesgourgues - 2010 - Dark energy with non-adiabatic sound speed initia.pdf}
}

@article{belliniHi_classBackgroundEvolution2020a,
  title = {Hi\_class: {{Background Evolution}}, {{Initial Conditions}} and {{Approximation Schemes}}},
  shorttitle = {Hi\_class},
  author = {Bellini, Emilio and Sawicki, Ignacy and Zumalac{\'a}rregui, Miguel},
  year = 2020,
  month = feb,
  journal = {JCAP},
  volume = {2020},
  number = {02},
  eprint = {1909.01828},
  primaryclass = {astro-ph, physics:gr-qc, physics:hep-ph, physics:hep-th},
  pages = {008--008},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2020/02/008},
  url = {http://arxiv.org/abs/1909.01828},
  urldate = {2024-09-09},
  abstract = {Cosmological datasets have great potential to elucidate the nature of dark energy and test gravity on the largest scales available to observation. Theoretical predictions can be computed with hi\_class (www.hiclass-code.net), an accurate, fast and flexible code for linear cosmology, incorporating a wide range of dark energy theories and modifications to general relativity. We introduce three new functionalities into hi\_class: (1) Support for models based on covariant Lagrangians, including a constraint-preserving integration scheme for the background evolution and a series of worked-out examples: Galileon, nKGB, quintessence (monomial, tracker) and Brans-Dicke. (2) Consistent initial conditions for the scalar-field perturbations in the deep radiation era, identifying the conditions under which modified-gravity isocurvature perturbations may grow faster than adiabatic modes leading to a loss of predictivity. (3) An automated quasistatic approximation scheme allowing order-of-magnitude improvement in computing performance without sacrificing accuracy for wide classes of models. These enhancements bring the treatment of dark energy and modified gravity models to the level of detail comparable to software tools restricted to standard {$\Lambda$}CDM cosmologies. The hi\_class code is publicly available (https://github.com/miguelzuma/hi\_class\_public), ready to explore current data and prepare for next-generation experiments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {High Energy Physics - Theory},
  file = {/home/hermasl/Zotero/storage/ZN88KCTT/Bellini et al. - 2020 - hi_class Background Evolution, Initial Conditions.pdf}
}

@misc{betancourtConceptualIntroductionHamiltonian2018,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  year = 2018,
  month = jul,
  number = {arXiv:1701.02434},
  eprint = {1701.02434},
  primaryclass = {stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1701.02434},
  urldate = {2024-04-03},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {/home/hermasl/Zotero/storage/XS74CW6Y/Betancourt - 2018 - A Conceptual Introduction to Hamiltonian Monte Car.pdf}
}

@misc{betancourtGeometricFoundationsHamiltonian2014,
  title = {The {{Geometric Foundations}} of {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, M. J. and Byrne, Simon and Livingstone, Samuel and Girolami, Mark},
  year = 2014,
  month = oct,
  number = {arXiv:1410.5110},
  eprint = {1410.5110},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1410.5110},
  url = {http://arxiv.org/abs/1410.5110},
  urldate = {2024-04-03},
  abstract = {Although Hamiltonian Monte Carlo has proven an empirical success, the lack of a rigorous theoretical understanding of the algorithm has in many ways impeded both principled developments of the method and use of the algorithm in practice. In this paper we develop the formal foundations of the algorithm through the construction of measures on smooth manifolds, and demonstrate how the theory naturally identifies efficient implementations and motivates promising generalizations.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/hermasl/Zotero/storage/3YFV2833/The Geometric Foundations of Hamiltonian Monte Carlo (Betancourt+, 2014).pdf;/home/hermasl/Zotero/storage/XKRY28T9/1410.html}
}

@article{bezansonJuliaFreshApproach2017a,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  shorttitle = {Julia},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  year = 2017,
  month = jan,
  journal = {SIAM Rev.},
  volume = {59},
  number = {1},
  eprint = {1411.1607},
  primaryclass = {cs},
  pages = {65--98},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/141000671},
  url = {https://epubs.siam.org/doi/10.1137/141000671},
  urldate = {2025-09-24},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/TJPX49TV/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf}
}

@article{birdMassiveNeutrinosNonlinear2012,
  title = {Massive {{Neutrinos}} and the {{Non-linear Matter Power Spectrum}}},
  author = {Bird, Simeon and Viel, Matteo and Haehnelt, Martin G.},
  year = 2012,
  month = mar,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {420},
  number = {3},
  eprint = {1109.4416},
  primaryclass = {astro-ph},
  pages = {2551--2561},
  issn = {00358711},
  doi = {10.1111/j.1365-2966.2011.20222.x},
  url = {http://arxiv.org/abs/1109.4416},
  urldate = {2025-11-11},
  abstract = {We perform an extensive suite of N-body simulations of the matter power spectrum, incorporating massive neutrinos in the range M = 0.15-0.6 eV, probing the non-linear regime at scales k {$<$} 10 hMpc-1 at z {$<$} 3. We extend the widely used HALOFIT approximation to account for the effect of massive neutrinos on the power spectrum. In the strongly non-linear regime HALOFIT systematically over-predicts the suppression due to the free-streaming of the neutrinos. The maximal discrepancy occurs at k \textasciitilde{} 1 hMpc-1, and is at the level of 10\% of the total suppression. Most published constraints on neutrino masses based on HALOFIT are not affected, as they rely on data probing the matter power spectrum in the linear or mildly non-linear regime. However, predictions for future galaxy, Lyman-alpha forest and weak lensing surveys extending to more non-linear scales will benefit from the improved approximation to the non-linear matter power spectrum we provide. Our approximation reproduces the induced neutrino suppression over the targeted scales and redshifts significantly better. We test its robustness with regard to changing cosmological parameters and a variety of modelling effects.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {/home/hermasl/Zotero/storage/TJIU2SD6/Bird et al. - 2012 - Massive Neutrinos and the Non-linear Matter Power Spectrum.pdf;/home/hermasl/Zotero/storage/7YD7TAAM/1109.html}
}

@article{blasCosmicLinearAnisotropy2011a,
  title = {The {{Cosmic Linear Anisotropy Solving System}} ({{CLASS}}) {{II}}: {{Approximation}} Schemes},
  shorttitle = {The {{Cosmic Linear Anisotropy Solving System}} ({{CLASS}}) {{II}}},
  author = {Blas, Diego and Lesgourgues, Julien and Tram, Thomas},
  year = 2011,
  month = jul,
  journal = {JCAP},
  volume = {07},
  eprint = {1104.2933},
  primaryclass = {astro-ph},
  pages = {034--034},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2011/07/034},
  url = {http://arxiv.org/abs/1104.2933},
  urldate = {2025-02-06},
  abstract = {Boltzmann codes are used extensively by several groups for constraining cosmological parameters with Cosmic Microwave Background and Large Scale Structure data. This activity is computationally expensive, since a typical project requires from 10'000 to 100'000 Boltzmann code executions. The newly released code CLASS (Cosmic Linear Anisotropy Solving System) incorporates improved approximation schemes leading to a simultaneous gain in speed and precision. We describe here the three approximations used by CLASS for basic LambdaCDM models, namely: a baryon-photon tight-coupling approximation which can be set to first order, second order or to a compromise between the two; an ultra-relativistic fluid approximation which had not been implemented in public distributions before; and finally a radiation streaming approximation taking reionisation into account.},
  archiveprefix = {arXiv},
  file = {/home/hermasl/Zotero/storage/5EHW7ZU8/The Cosmic Linear Anisotropy Solving System (CLASS) II (Blas+, 2011).pdf;/home/hermasl/Zotero/storage/XJC8WCNX/1104.html}
}

@misc{bollietHighaccuracyEmulatorsObservables2023,
  title = {High-Accuracy Emulators for Observables in \${{$\Lambda$}}\${{CDM}}, \${{N}}\_\textbackslash mathrm\textbraceleft eff\textbraceright\$, \${{$\Sigma$m}}\_{$\nu\$$}, and \$w\$ Cosmologies},
  author = {Bolliet, Boris and Mancini, Alessio Spurio and Hill, J. Colin and Madhavacheril, Mathew and Jense, Hidde T. and Calabrese, Erminia and Dunkley, Jo},
  year = 2023,
  month = mar,
  number = {arXiv:2303.01591},
  eprint = {2303.01591},
  primaryclass = {astro-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.01591},
  url = {http://arxiv.org/abs/2303.01591},
  urldate = {2025-12-18},
  abstract = {We use the emulation framework CosmoPower to construct and publicly release neural network emulators of cosmological observables, including the Cosmic Microwave Background (CMB) temperature and polarization power spectra, matter power spectrum, distance-redshift relation, baryon acoustic oscillation (BAO) and redshift-space distortion (RSD) observables, and derived parameters. We train our emulators on Einstein-Boltzmann calculations obtained with high-precision numerical convergence settings, for a wide range of cosmological models including {$\Lambda$}CDM, {$w$}CDM, {$\Lambda$}CDM+{$N$}eff, and {$\Lambda$}CDM+{$\Sigmam\nu$}. Our CMB emulators are accurate to better than 0.5\% out to {$\ell$} = 104 which is sufficient for Stage-IV data analysis, and our {$P^{1}k$}º emulators reach the same accuracy level out to {$k$} = 50 Mpc 1, which is sufficient for Stage-III data analysis. We release the emulators via an online repository (CosmoPower Organisation), which will be continually updated with additional extended cosmological models. Our emulators accelerate cosmological data analysis by orders of magnitude, enabling cosmological parameter extraction analyses, using current survey data, to be performed on a laptop. We validate our emulators by comparing them to class and camb and by reproducing cosmological parameter constraints derived from Planck TT, TE, EE, and CMB lensing data, as well as from the Atacama Cosmology Telescope Data Release 4 CMB data, Dark Energy Survey Year-1 galaxy lensing and clustering data, and Baryon Oscillation Spectroscopic Survey Data Release 12 BAO and RSD data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {/home/hermasl/Zotero/storage/JJB3SHCR/Bolliet et al. - 2023 - High-accuracy emulators for observables in $Λ$CDM, $N_mathrm eff $, $Σm_ν$, and $w$ cosmologies.pdf}
}

@article{bull$L$CDMProblemsSolutions2016,
  title = {Beyond \${{$\Lambda$}}\${{CDM}}: {{Problems}}, Solutions, and the Road Ahead},
  shorttitle = {Beyond \${{$\Lambda$}}\${{CDM}}},
  author = {Bull, Philip and Akrami, Yashar and Adamek, Julian and Baker, Tessa and Bellini, Emilio and Jim{\'e}nez, Jose Beltr{\'a}n and Bentivegna, Eloisa and Camera, Stefano and Clesse, S{\'e}bastien and Davis, Jonathan H. and Dio, Enea Di and Enander, Jonas and Heavens, Alan and Heisenberg, Lavinia and Hu, Bin and Llinares, Claudio and Maartens, Roy and M{\"o}rtsell, Edvard and Nadathur, Seshadri and Noller, Johannes and Pasechnik, Roman and Pawlowski, Marcel S. and Pereira, Thiago S. and Quartin, Miguel and Ricciardone, Angelo and {Riemer-S{\o}rensen}, Signe and Rinaldi, Massimiliano and Sakstein, Jeremy and Saltas, Ippocratis D. and Salzano, Vincenzo and Sawicki, Ignacy and Solomon, Adam R. and Spolyar, Douglas and Starkman, Glenn D. and Steer, Dani{\`e}le and Tereno, Ismael and Verde, Licia and {Villaescusa-Navarro}, Francisco and von Strauss, Mikael and Winther, Hans A.},
  year = 2016,
  month = jun,
  journal = {Physics of the Dark Universe},
  volume = {12},
  eprint = {1512.05356},
  primaryclass = {astro-ph},
  pages = {56--99},
  issn = {22126864},
  doi = {10.1016/j.dark.2016.02.001},
  url = {http://arxiv.org/abs/1512.05356},
  urldate = {2024-11-08},
  abstract = {Despite its continued observational successes, there is a persistent (and growing) interest in extending cosmology beyond the standard model, \$\textbackslash Lambda\$CDM. This is motivated by a range of apparently serious theoretical issues, involving such questions as the cosmological constant problem, the particle nature of dark matter, the validity of general relativity on large scales, the existence of anomalies in the CMB and on small scales, and the predictivity and testability of the inflationary paradigm. In this paper, we summarize the current status of \$\textbackslash Lambda\$CDM as a physical theory, and review investigations into possible alternatives along a number of different lines, with a particular focus on highlighting the most promising directions. While the fundamental problems are proving reluctant to yield, the study of alternative cosmologies has led to considerable progress, with much more to come if hopes about forthcoming high-precision observations and new theoretical ideas are fulfilled.},
  archiveprefix = {arXiv},
  keywords = {High Energy Physics - Theory},
  file = {/home/hermasl/Zotero/storage/R55E8KQI/Beyond $Λ$CDM (Bull+, 2016).pdf;/home/hermasl/Zotero/storage/R86ZMBP5/1512.html}
}

@misc{callinHowCalculateCMB2006a,
  title = {How to Calculate the {{CMB}} Spectrum},
  author = {Callin, Petter},
  year = 2006,
  month = jun,
  number = {arXiv:astro-ph/0606683},
  eprint = {astro-ph/0606683},
  publisher = {arXiv},
  doi = {10.48550/arXiv.astro-ph/0606683},
  url = {http://arxiv.org/abs/astro-ph/0606683},
  urldate = {2024-12-03},
  abstract = {We present a self-contained description of everything needed to write a program that calculates the CMB power spectrum for the standard model of cosmology (LCDM). This includes the equations used, assumptions and approximations imposed on their solutions, and most importantly the algorithms and programming tricks needed to make the code actually work. The resulting program is compared to CMBFAST and typically agrees to within 0.1\% - 0.4\%. It includes both helium, reionization, neutrinos and the polarization power spectrum. The methods presented here could serve as a starting point for people wanting to write their own CMB program from scratch, for instance to look at more exotic cosmological models where CMBFAST or the other standard programs can't be used directly.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics},
  file = {/home/hermasl/Zotero/storage/VN26L9HI/How to calculate the CMB spectrum (Callin, 2006).pdf;/home/hermasl/Zotero/storage/C4XHDEGH/0606683.html}
}

@article{campagneJAXCOSMOEndtoEndDifferentiable2023,
  title = {{{JAX-COSMO}}: {{An End-to-End Differentiable}} and {{GPU Accelerated Cosmology Library}}},
  shorttitle = {{{JAX-COSMO}}},
  author = {Campagne, Jean-Eric and Lanusse, Fran{\c c}ois and Zuntz, Joe and Boucaud, Alexandre and Casas, Santiago and Karamanis, Minas and Kirkby, David and Lanzieri, Denise and Li, Yin and Peel, Austin},
  year = 2023,
  month = apr,
  journal = {TheOJA},
  volume = {6},
  eprint = {2302.05163},
  primaryclass = {astro-ph},
  pages = {10.21105/astro.2302.05163},
  issn = {2565-6120},
  doi = {10.21105/astro.2302.05163},
  url = {http://arxiv.org/abs/2302.05163},
  urldate = {2024-03-01},
  abstract = {We present jax-cosmo, a library for automatically differentiable cosmological theory calculations. It uses the JAX library, which has created a new coding ecosystem, especially in probabilistic programming. As well as batch acceleration, just-in-time compilation, and automatic optimization of code for different hardware modalities (CPU, GPU, TPU), JAX exposes an automatic differentiation (autodiff) mechanism. Thanks to autodiff, jax-cosmo gives access to the derivatives of cosmological likelihoods with respect to any of their parameters, and thus enables a range of powerful Bayesian inference algorithms, otherwise impractical in cosmology, such as Hamiltonian Monte Carlo and Variational Inference. In its initial release, jax-cosmo implements background evolution, linear and non-linear power spectra (using halofit or the Eisenstein and Hu transfer function), as well as angular power spectra with the Limber approximation for galaxy and weak lensing probes, all differentiable with respect to the cosmological parameters and their other inputs. We illustrate how autodiff can be a game-changer for common tasks involving Fisher matrix computations, or full posterior inference with gradient-based techniques. In particular, we show how Fisher matrices are now fast, exact, no longer require any fine tuning, and are themselves differentiable. Finally, using a Dark Energy Survey Year 1 3x2pt analysis as a benchmark, we demonstrate how jax-cosmo can be combined with Probabilistic Programming Languages to perform posterior inference with state-of-the-art algorithms including a No U-Turn Sampler, Automatic Differentiation Variational Inference,and Neural Transport HMC. We further demonstrate that Normalizing Flows using Neural Transport are a promising methodology for model validation in the early stages of analysis.},
  archiveprefix = {arXiv},
  keywords = {_tablet},
  file = {/home/hermasl/Zotero/storage/23QMCRVI/JAX-COSMO (Campagne+, 2023).pdf;/home/hermasl/Zotero/storage/PK7CB4FM/2302.html}
}

@article{casasCLAPPCLASSLLM2025,
  title = {{{CLAPP}}: {{The CLASS LLM Agent}} for {{Pair Programming}}},
  shorttitle = {{{CLAPP}}},
  author = {Casas, Santiago and Fidler, Christian and Bolliet, Boris and {Villaescusa-Navarro}, Francisco and Lesgourgues, Julien},
  year = 2025,
  month = aug,
  journal = {ArXiv},
  eprint = {2508.05728},
  primaryclass = {astro-ph},
  doi = {10.48550/arXiv.2508.05728},
  url = {http://arxiv.org/abs/2508.05728},
  urldate = {2025-08-11},
  abstract = {We introduce CLAPP (CLASS LLM Agent for Pair Programming), an interactive AI assistant designed to support researchers working with the Einstein-Boltzmann solver CLASS. CLAPP leverages large language models (LLMs) and domain-specific retrieval to provide conversational coding support for CLASS-answering questions, generating code, debugging errors, and producing plots. Its architecture combines multi-agent LLM orchestration, semantic search across CLASS documentation, and a live Python execution environment. Deployed as a user-friendly web application, CLAPP lowers the entry barrier for scientists unfamiliar with AI tools and enables more productive human-AI collaboration in computational and numerical cosmology. The app is available at https://classclapp.streamlit.app},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/home/hermasl/Zotero/storage/ARVW8Y6N/CLAPP (Casas+, 2025).pdf;/home/hermasl/Zotero/storage/NGJQF5JT/2508.html}
}

@article{chevallierAcceleratingUniversesScaling2001,
  title = {Accelerating {{Universes}} with {{Scaling Dark Matter}}},
  author = {Chevallier, M. and Polarski, D.},
  year = 2001,
  month = apr,
  journal = {Int. J. Mod. Phys. D},
  volume = {10},
  number = {02},
  eprint = {gr-qc/0009008},
  pages = {213--223},
  issn = {0218-2718, 1793-6594},
  doi = {10.1142/S0218271801000822},
  url = {http://arxiv.org/abs/gr-qc/0009008},
  urldate = {2025-07-04},
  abstract = {Friedmann-Robertson-Walker universes with a presently large fraction of the energy density stored in an X-component with wX {$<$} -1/3, are considered. We find all the critical points of the system for constant equations of state in that range. We consider further several background quantities that can distinguish the models with different wX values. Using a simple toy model with a varying equation of state, we show that even a large variation of wX at small redshifts is very difficult to observe with dL(z) measurements up to z {$\sim$} 1. Therefore, it will require accurate measurements in the range 1 {$<$} z {$<$} 2 and independent accurate knowledge of \textohm m,0 (and/or \textohm X,0) in order to resolve a variable wX from a constant wX .},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Astrophysics},
  file = {/home/hermasl/Zotero/storage/43NZHTBA/Chevallier and Polarski - 2001 - Accelerating Universes with Scaling Dark Matter.pdf}
}

@misc{chiarenzaBLASTLimberAngular2024,
  title = {{{BLAST}}: {{Beyond Limber Angular}} Power {{Spectra Toolkit}}. {{A}} Fast and Efficient Algorithm for 3x2 Pt Analysis},
  shorttitle = {{{BLAST}}},
  author = {Chiarenza, Sofia and Bonici, Marco and Percival, Will and White, Martin},
  year = 2024,
  month = oct,
  number = {arXiv:2410.03632},
  eprint = {2410.03632},
  primaryclass = {astro-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.03632},
  url = {http://arxiv.org/abs/2410.03632},
  urldate = {2024-12-04},
  abstract = {The advent of next-generation photometric and spectroscopic surveys is approaching, bringing more data with tighter error bars. As a result, theoretical models will become more complex, incorporating additional parameters, which will increase the dimensionality of the parameter space and make posteriors more challenging to explore. Consequently, the need to improve and speed up our current analysis pipelines will grow. In this work, we focus on the 3 \texttimes{} 2 pt statistics, a summary statistic that has become increasingly popular in recent years due to its great constraining power. These statistics involve calculating angular two-point correlation functions for the auto- and cross-correlations between galaxy clustering and weak lensing. The corresponding model is determined by integrating the product of the power spectrum and two highly-oscillating Bessel functions over three dimensions, which makes the evaluation particularly challenging. Typically, this difficulty is circumvented by employing the so-called Limber approximation, which is an important source of error. We present Blast.jl, an innovative and efficient algorithm for calculating angular power spectra without employing the Limber approximation or assuming a scale-dependent growth rate, based on the use of Chebyshev polynomials. The algorithm is compared with the publicly available beyond-Limber codes, whose performances were recently tested by the LSST Dark Energy Science Collaboration in the N5K challenge. At similar accuracy, Blast.jl is {$\approx$} 10-15\texttimes{} faster than the winning method of the challenge, also showing excellent scaling with respect to various hyper-parameters.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/Q9PF8ZVL/Chiarenza et al. - 2024 - BLAST Beyond Limber Angular power Spectra Toolkit.pdf}
}

@article{chlubaRadiativeTransferEffects2012,
  title = {Radiative Transfer Effects during Primordial Helium Recombination},
  author = {Chluba, Jens and Fung, Jeffrey and Switzer, Eric R.},
  year = 2012,
  month = jul,
  journal = {MNRAS},
  volume = {423},
  number = {4},
  eprint = {1110.0247},
  primaryclass = {astro-ph},
  pages = {3227--3242},
  issn = {00358711},
  doi = {10.1111/j.1365-2966.2012.21110.x},
  url = {http://arxiv.org/abs/1110.0247},
  urldate = {2025-09-15},
  abstract = {In this paper we refine the calculation of primordial helium recombination, accounting for several additional effects that were neglected or treated more approximately in previous studies. These include consideration of (i) time-dependent radiative transfer interaction between the 2\textasciicircum 1 P\_1 - 1\textasciicircum 1 S\_0 and 2\textasciicircum 3 P\_1 - 1\textasciicircum 1 S\_0 resonances; (ii) time-dependent radiative transfer for the partially overlapping n\textasciicircum 1 P\_1 - 1\textasciicircum 1 S\_0, n\textasciicircum 1 D\_2 - 1\textasciicircum 1 S\_0 and n\textasciicircum 3 P\_1 - 1\textasciicircum 1 S\_0 series with 3 {$<$}= n {$<$}= 10; (iii) electron scattering within a kernel approach. We also briefly discuss the effect of electron scattering and HI quadrupole lines on the recombination of hydrogen. Although the physics of all the considered processes is interesting and subtle, for the standard cosmology, with Y\_p \textbackslash sim 0.24, the overall correction to the ionization history during helium recombination with respect to the previous implementation of CosmoRec remains smaller than \textbar DeltaNe/Ne\textbar{} \textbackslash sim 0.05\%. The dominant improvement is caused by consistent inclusion of resonance scattering for the 2\textasciicircum 1 P\_1 - 1\textasciicircum 1 S\_0 resonance. For cosmologies with a large helium fraction, Y\_p \textbackslash sim 0.4, the difference reaches \textbar D N\_e/N\_e\textbar\textbackslash sim 0.22\% at z\textbackslash sim1800, however, the overall correction to the CMB power spectra is small, exceeding \textbar D C\_l/C\_l\textbar{} \textbackslash sim 0.05 \% only at l{$>\backslash$}sim 3000. In comparison to the current version of Recfast the difference reaches \textbar D C\_l/C\_l\textbar\textbackslash sim0.4\% at l\textbackslash sim 3000 for Y\_p \textbackslash sim 0.4, and also for the standard value Y\_p \textbackslash sim 0.24 we find differences \textbar DC\_l/C\_l\textbar{$>\backslash$}sim 0.1\% at l\textbackslash sim 2500. The new processes are now included by the cosmological recombination code CosmoRec and can be activated as needed for most settings without affecting its runtime significantly.},
  archiveprefix = {arXiv},
  file = {/home/hermasl/Zotero/storage/X55EWXX6/Chluba et al. - 2012 - Radiative transfer effects during primordial helium recombination.pdf;/home/hermasl/Zotero/storage/IJDBSKBU/1110.html}
}

@article{chudaykinNonlinearPerturbationTheory2020,
  title = {Non-Linear Perturbation Theory Extension of the {{Boltzmann}} Code {{CLASS}}},
  author = {Chudaykin, Anton and Ivanov, Mikhail M. and Philcox, Oliver H. E. and Simonovi{\'c}, Marko},
  year = 2020,
  month = sep,
  journal = {Phys. Rev. D},
  volume = {102},
  number = {6},
  eprint = {2004.10607},
  primaryclass = {astro-ph},
  pages = {063533},
  issn = {2470-0010, 2470-0029},
  doi = {10.1103/PhysRevD.102.063533},
  url = {http://arxiv.org/abs/2004.10607},
  urldate = {2025-11-10},
  abstract = {We present a new open-source code that calculates one-loop power auto- and cross-power spectra for matter fields and biased tracers in real and redshift space. These spectra incorporate all ingredients required for a direct application to data: non-linear bias, redshift-space distortions, infra-red resummation, counterterms, and the Alcock-Paczynski effect. Our code is based on the Boltzmann solver CLASS and inherits its advantage: user friendliness, ease of modification, high speed, and simple interface with other software. We present detailed descriptions of the theoretical model, the code structure, approximations, and accuracy tests. A typical end-to-end run for one cosmology takes \$\textbackslash sim 0.3\$ seconds, which is sufficient for Markov Chain Monte Carlo parameter extraction. As an example, we apply the code to data from the Baryon Oscillation Spectroscopic Survey (BOSS) and infer cosmological parameters from the shape of the galaxy power spectrum.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {/home/hermasl/Zotero/storage/6YQ9LPCW/Chudaykin et al. - 2020 - Non-linear perturbation theory extension of the Boltzmann code CLASS.pdf}
}

@article{cliftonModifiedGravityCosmology2012a,
  title = {Modified {{Gravity}} and {{Cosmology}}},
  author = {Clifton, Timothy and Ferreira, Pedro G. and Padilla, Antonio and Skordis, Constantinos},
  year = 2012,
  month = mar,
  journal = {Physics Reports},
  volume = {513},
  number = {1-3},
  eprint = {1106.2476},
  primaryclass = {astro-ph, physics:gr-qc, physics:hep-th},
  pages = {1--189},
  issn = {03701573},
  doi = {10.1016/j.physrep.2012.01.001},
  url = {http://arxiv.org/abs/1106.2476},
  urldate = {2024-09-09},
  abstract = {In this review we present a thoroughly comprehensive survey of recent work on modified theories of gravity and their cosmological consequences. Amongst other things, we cover General Relativity, Scalar-Tensor, Einstein-Aether, and Bimetric theories, as well as TeVeS, f(R), general higher-order theories, Horava-Lifschitz gravity, Galileons, Ghost Condensates, and models of extra dimensions including Kaluza-Klein, Randall-Sundrum, DGP, and higher co-dimension braneworlds. We also review attempts to construct a Parameterised Post-Friedmannian formalism, that can be used to constrain deviations from General Relativity in cosmology, and that is suitable for comparison with data on the largest scales. These subjects have been intensively studied over the past decade, largely motivated by rapid progress in the field of observational cosmology that now allows, for the first time, precision tests of fundamental physics on the scale of the observable Universe. The purpose of this review is to provide a reference tool for researchers and students in cosmology and gravitational physics, as well as a self-contained, comprehensive and up-to-date introduction to the subject as a whole.},
  archiveprefix = {arXiv},
  keywords = {High Energy Physics - Theory},
  file = {/home/hermasl/Zotero/storage/V7RWIRT3/Modified Gravity and Cosmology (Clifton+, 2012).pdf;/home/hermasl/Zotero/storage/V6R3WPTU/1106.html}
}

@article{collaborationEuclidPreparationSensitivity2025,
  title = {Euclid Preparation. {{Sensitivity}} to Neutrino Parameters},
  author = {Collaboration, Euclid and Archidiacono, M. and Lesgourgues, J. and Casas, S. and Pamuk, S. and Sch{\"o}neberg, N. and Sakr, Z. and Parimbelli, G. and Schneider, A. and Peters, F. Hervas and Pace, F. and Sabarish, V. M. and Costanzi, M. and Camera, S. and Carbone, C. and Clesse, S. and Frusciante, N. and Fumagalli, A. and Monaco, P. and Scott, D. and Viel, M. and Amara, A. and Andreon, S. and Auricchio, N. and Baldi, M. and Bardelli, S. and Bodendorf, C. and Bonino, D. and Branchini, E. and Brescia, M. and Brinchmann, J. and Capobianco, V. and Cardone, V. F. and Carretero, J. and Castellano, M. and Cavuoti, S. and Cimatti, A. and Congedo, G. and Conselice, C. J. and Conversi, L. and Copin, Y. and Courbin, F. and Courtois, H. M. and Silva, A. Da and Degaudenzi, H. and Douspis, M. and Dubath, F. and Duncan, C. A. J. and Dupac, X. and Dusini, S. and Ealet, A. and Farina, M. and Farrens, S. and Ferriol, S. and Frailis, M. and Franceschi, E. and Galeotta, S. and Gillis, B. and Giocoli, C. and Grazian, A. and Grupp, F. and Guzzo, L. and Haugan, S. V. H. and Hoekstra, H. and Hormuth, F. and Hornstrup, A. and Jahnke, K. and Joachimi, B. and Keih{\"a}nen, E. and Kermiche, S. and Kiessling, A. and Kilbinger, M. and Kitching, T. and Kubik, B. and Kunz, M. and {Kurki-Suonio}, H. and Ligori, S. and Lilje, P. B. and Lindholm, V. and Lloro, I. and Maino, D. and Maiorano, E. and Mansutti, O. and Marggraf, O. and Markovic, K. and Martinet, N. and Marulli, F. and Massey, R. and Maurogordato, S. and McCracken, H. J. and Medinaceli, E. and Mei, S. and Mellier, Y. and Meneghetti, M. and Merlin, E. and Meylan, G. and Moresco, M. and Moscardini, L. and Munari, E. and Niemi, S.-M. and Nightingale, J. W. and Padilla, C. and Paltani, S. and Pasian, F. and Pedersen, K. and Percival, W. J. and Pettorino, V. and Pires, S. and Polenta, G. and Poncet, M. and Popa, L. A. and Pozzetti, L. and Raison, F. and Rebolo, R. and Renzi, A. and Rhodes, J. and Riccio, G. and Romelli, E. and Roncarelli, M. and Saglia, R. and Sapone, D. and Sartoris, B. and Scaramella, R. and Schirmer, M. and Schneider, P. and Schrabback, T. and Secroun, A. and Seidel, G. and Serrano, S. and Sirignano, C. and Sirri, G. and Stanco, L. and {Tallada-Cresp{\'i}}, P. and Taylor, A. N. and Tereno, I. and {Toledo-Moreo}, R. and Torradeflot, F. and Tutusaus, I. and Valenziano, L. and Vassallo, T. and Veropalumbo, A. and Wang, Y. and Weller, J. and Zamorani, G. and Zoubian, J. and Zucca, E. and Biviano, A. and Boucaud, A. and Bozzo, E. and Burigana, C. and Calabrese, M. and {Colodro-Conde}, C. and Crocce, M. and Fabbian, G. and {Graci{\'a}-Carpio}, J. and Mainetti, G. and Martinelli, M. and Mauri, N. and Neissner, C. and Scottez, V. and Tenti, M. and Wiesmann, M. and Akrami, Y. and Anselmi, S. and Baccigalupi, C. and Ballardini, M. and Bernardeau, F. and Bertacca, D. and Borgani, S. and Borsato, E. and Bruton, S. and Cabanac, R. and Cappi, A. and Carvalho, C. S. and Castignani, G. and Castro, T. and {Ca{\~n}as-Herrera}, G. and Chambers, K. C. and Contarini, S. and Cooray, A. R. and Coupon, J. and Davini, S. and de la Torre, S. and Lucia, G. De and Desprez, G. and Domizio, S. Di and {D{\'i}az-S{\'a}nchez}, A. and Vigo, J. A. Escartin and Escoffier, S. and Ferreira, P. G. and Ferrero, I. and Finelli, F. and Gabarra, L. and Ganga, K. and {Garc{\'i}a-Bellido}, J. and Gaztanaga, E. and Giacomini, F. and Gozaliasl, G. and Gregorio, A. and Hall, A. and Hildebrandt, H. and Ili{\'c}, S. and Kajava, J. J. E. and Kansal, V. and Karagiannis, D. and Kirkpatrick, C. C. and Legrand, L. and Loureiro, A. and {Macias-Perez}, J. and Maggio, G. and Magliocchetti, M. and Mannucci, F. and Maoli, R. and Martins, C. J. A. P. and Matthew, S. and Maurin, L. and Metcalf, R. B. and Migliaccio, M. and Morgante, G. and Nadathur, S. and Walton, Nicholas A. and Patrizii, L. and Pezzotta, A. and P{\"o}ntinen, M. and Popa, V. and Porciani, C. and Potter, D. and Reimberg, P. and Risso, I. and Rocci, P.-F. and Sahl{\'e}n, M. and S{\'a}nchez, A. G. and Sefusatti, E. and Sereno, M. and Simon, P. and Mancini, A. Spurio and Steinwagner, J. and Testera, G. and Tewes, M. and Teyssier, R. and Toft, S. and Tosi, S. and Troja, A. and Tucci, M. and Valieri, C. and Valiviita, J. and Vergani, D. and Verza, G. and Vielzeuf, P.},
  year = 2025,
  month = jan,
  journal = {A\&A},
  volume = {693},
  eprint = {2405.06047},
  primaryclass = {astro-ph},
  pages = {A58},
  issn = {0004-6361, 1432-0746},
  doi = {10.1051/0004-6361/202450859},
  url = {http://arxiv.org/abs/2405.06047},
  urldate = {2025-09-08},
  abstract = {Methods. We compare the accuracy of different algorithms predicting the nonlinear matter power spectrum for such models. We then validate several pipelines for Fisher matrix and Markov Chain Monte Carlo (MCMC) forecasts, using different theory codes, algorithms for numerical derivatives, and assumptions concerning the non-linear cut-off scale. Results. The Euclid primary probes alone will reach a sensitivity of {$\sigma$}( m{$\nu$} = 60 meV) = 56 meV in the {$\Lambda$}CDM+ m{$\nu$} model, whereas the combination with cosmic microwave background (CMB) data from Planck is expected to achieve {$\sigma$}( m{$\nu$} ) = 23 meV and raise the evidence for a non-zero neutrino mass to at least the 2.6 {$\sigma$} level. This can be pushed to a 4 {$\sigma$} detection if future CMB data from LiteBIRD and CMB Stage-IV are included. In combination with Planck , Euclid will also deliver tight constraints on ∆Neff {$<$} 0.144 (95\%CL) in the {$\Lambda$}CDM+ m{$\nu$} +Neff model, or ∆Neff {$<$} 0.063 when future CMB data are included. When floating the dark energy parameters, we find that the sensitivity to Neff remains stable, while that to m{$\nu$} degrades at most by a factor two. Conclusions. This work illustrates the complementarity between the Euclid spectroscopic and imaging/photometric surveys and between Euclid and CMB constraints. Euclid will have a great potential for measuring the neutrino mass and excluding wellmotivated scenarios with additional relativistic particles.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/VG43KA55/Collaboration et al. - 2025 - Euclid preparation. Sensitivity to neutrino parame.pdf}
}

@article{collaborationPlanckIntermediateResults2014,
  title = {Planck Intermediate Results. {{XVI}}. {{Profile}} Likelihoods for Cosmological Parameters},
  author = {Collaboration, Planck and Ade, P. A. R. and Aghanim, N. and Arnaud, M. and Ashdown, M. and Aumont, J. and Baccigalupi, C. and Banday, A. J. and Barreiro, R. B. and Bartlett, J. G. and Battaner, E. and Benabed, K. and {Benoit-L{\'e}vy}, A. and Bernard, J.-P. and Bersanelli, M. and Bielewicz, P. and Bobin, J. and Bonaldi, A. and Bond, J. R. and Bouchet, F. R. and Burigana, C. and Cardoso, J.-F. and Catalano, A. and Chamballu, A. and Chiang, H. C. and Christensen, P. R. and Clements, D. L. and Colombi, S. and Colombo, L. P. L. and Couchot, F. and Cuttaia, F. and Danese, L. and Davis, R. J. and de Bernardis, P. and de Rosa, A. and de Zotti, G. and Delabrouille, J. and Dickinson, C. and Diego, J. M. and Dole, H. and Donzelli, S. and Dor{\'e}, O. and Douspis, M. and Dupac, X. and En{\ss}lin, T. A. and Eriksen, H. K. and Finelli, F. and Forni, O. and Frailis, M. and Franceschi, E. and Galeotta, S. and Galli, S. and Ganga, K. and Giard, M. and {Giraud-H{\'e}raud}, Y. and {Gonz{\'a}lez-Nuevo}, J. and G{\'o}rski, K. M. and Gregorio, A. and Gruppuso, A. and Hansen, F. K. and Harrison, D. and {Henrot-Versill{\'e}}, S. and {Hern{\'a}ndez-Monteagudo}, C. and Herranz, D. and Hildebrandt, S. R. and Hivon, E. and Hobson, M. and Holmes, W. A. and Hornstrup, A. and Hovest, W. and Huffenberger, K. M. and Jaffe, A. H. and Jaffe, T. R. and Jones, W. C. and Juvela, M. and Keih{\"a}nen, E. and Keskitalo, R. and Kisner, T. S. and Kneissl, R. and Knoche, J. and Knox, L. and Kunz, M. and {Kurki-Suonio}, H. and Lagache, G. and L{\"a}hteenm{\"a}ki, A. and Lamarre, J.-M. and Lasenby, A. and Leonardi, R. and Liddle, A. and Liguori, M. and Lilje, P. B. and {Linden-V{\o}rnle}, M. and {L{\'o}pez-Caniego}, M. and Lubin, P. M. and {Mac{\'i}as-P{\'e}rez}, J. F. and Maffei, B. and Maino, D. and Mandolesi, N. and Maris, M. and Martin, P. G. and {Mart{\'i}nez-Gonz{\'a}lez}, E. and Masi, S. and Massardi, M. and Matarrese, S. and Mazzotta, P. and Melchiorri, A. and Mendes, L. and Mennella, A. and Migliaccio, M. and Mitra, S. and {Miville-Desch{\^e}nes}, M.-A. and Moneti, A. and Montier, L. and Morgante, G. and Munshi, D. and Murphy, J. A. and Naselsky, P. and Nati, F. and Natoli, P. and Noviello, F. and Novikov, D. and Novikov, I. and Oxborrow, C. A. and Pagano, L. and Pajot, F. and Paoletti, D. and Pasian, F. and Perdereau, O. and Perotto, L. and Perrotta, F. and Pettorino, V. and Piacentini, F. and Piat, M. and Pierpaoli, E. and Pietrobon, D. and Plaszczynski, S. and Pointecouteau, E. and Polenta, G. and Popa, L. and Pratt, G. W. and Puget, J.-L. and Rachen, J. P. and Rebolo, R. and Reinecke, M. and Remazeilles, M. and Renault, C. and Ricciardi, S. and Riller, T. and Ristorcelli, I. and Rocha, G. and Rosset, C. and Roudier, G. and {d'Orfeuil}, B. Rouill{\'e} and {Rubi{\~n}o-Mart{\'i}n}, J. A. and Rusholme, B. and Sandri, M. and Savelainen, M. and Savini, G. and Spencer, L. D. and Spinelli, M. and Starck, J.-L. and Sureau, F. and Sutton, D. and {Suur-Uski}, A.-S. and Sygnet, J.-F. and Tauber, J. A. and Terenzi, L. and Toffolatti, L. and Tomasi, M. and Tristram, M. and Tucci, M. and Umana, G. and Valenziano, L. and Valiviita, J. and Tent, B. Van and Vielva, P. and Villa, F. and Wade, L. A. and Wandelt, B. D. and White, M. and Yvon, D. and Zacchei, A. and Zonca, A.},
  year = 2014,
  month = jun,
  journal = {A\&A},
  volume = {566},
  eprint = {1311.1657},
  primaryclass = {astro-ph},
  pages = {A54},
  issn = {0004-6361, 1432-0746},
  doi = {10.1051/0004-6361/201323003},
  url = {http://arxiv.org/abs/1311.1657},
  urldate = {2025-08-14},
  abstract = {Abstract: We explore the 2013 Planck likelihood function with a high-precision multi-dimensional minimizer (Minuit). This allows a refinement of the {$\Lambda$}CDM best-fit solution with respect to previously-released results, and the construction of frequentist confidence intervals using profile likelihoods. The agreement with the cosmological results from the Bayesian framework is excellent, demonstrating the robustness of the Planck results to the statistical methodology. We investigate the inclusion of neutrino masses, where more significant differences may appear due to the non-Gaussian nature of the posterior mass distribution. By applying the Feldman--Cousins prescription, we again obtain results very similar to those of the Bayesian methodology. However, the profile-likelihood analysis of the CMB combination (Planck+WP+highL) reveals a minimum well within the unphysical negative-mass region. We show that inclusion of the Planck CMB-lensing information regularizes this issue, and provide a robust frequentist upper limit m{$\nu$} {$\leq$} 0.26 eV (95\% confidence) from the CMB+lensing+BAO data combination.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/SWG5EHY2/Collaboration et al. - 2014 - Planck intermediate results. XVI. Profile likeliho.pdf}
}

@article{cyr-racinePhotonsBaryonsAtoms2011,
  title = {Photons and {{Baryons}} before {{Atoms}}: {{Improving}} the {{Tight-Coupling Approximation}}},
  shorttitle = {Photons and {{Baryons}} before {{Atoms}}},
  author = {{Cyr-Racine}, Francis-Yan and Sigurdson, Kris},
  year = 2011,
  month = may,
  journal = {Phys. Rev. D},
  volume = {83},
  number = {10},
  eprint = {1012.0569},
  primaryclass = {astro-ph},
  pages = {103521},
  issn = {1550-7998, 1550-2368},
  doi = {10.1103/PhysRevD.83.103521},
  url = {http://arxiv.org/abs/1012.0569},
  urldate = {2025-02-19},
  abstract = {Prior to recombination photons, electrons, and atomic nuclei rapidly scattered and behaved, almost, like a single tightly-coupled photon-baryon plasma. We investigate here the accuracy of the tight-coupling approximation commonly used to numerically evolve the baryon and photon perturbation equations at early times. By solving the exact perturbations equations with a stiff solver starting deep in the radiation-dominated epoch we find the level of inaccuracy introduced by resorting to the standard first-order tight-coupling approximation. We develop a new second-order approximation in the inverse Thomson opacity expansion and show that it closely tracks the full solution, at essentially no extra numerical cost. We find the bias on estimates of cosmological parameters introduced by the first-order approximation is, for most parameters, negligible. Finally, we show that our second-order approximation can be used to reduce the time needed to compute cosmic microwave background angular spectra by as much as \textasciitilde 17\%.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/J3KCV28X/Cyr-Racine and Sigurdson - 2011 - Photons and Baryons before Atoms Improving the Ti.pdf}
}

@article{damicoLimits$w$CDMEFTofLSS2021,
  title = {Limits on \$w\${{CDM}} from the {{EFTofLSS}} with the {{PyBird}} Code},
  author = {D'Amico, Guido and Senatore, Leonardo and Zhang, Pierre},
  year = 2021,
  month = jan,
  journal = {J. Cosmol. Astropart. Phys.},
  volume = {2021},
  number = {01},
  eprint = {2003.07956},
  primaryclass = {astro-ph},
  pages = {006--006},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2021/01/006},
  url = {http://arxiv.org/abs/2003.07956},
  urldate = {2025-11-10},
  abstract = {We apply the Effective Field Theory of Large-Scale Structure to analyze the \$w\$CDM cosmological model. By using the full shape of the power spectrum and the BAO post-reconstruction measurements from BOSS, the Supernovae from Pantheon, and a prior from BBN, we set the competitive CMB-independent limit \$w=-1.046\_\textbraceleft -0.052\textbraceright\textasciicircum\textbraceleft +0.055\textbraceright\$ at \$68\textbackslash\%\$ C.L.. After adding the Planck CMB data, we find \$w=-1.023\_\textbraceleft -0.030\textbraceright\textasciicircum\textbraceleft +0.033\textbraceright\$ at \$68\textbackslash\%\$ C.L.. Our results are obtained using PyBird, a new, fast Python-based code which we make publicly available.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics,General Relativity and Quantum Cosmology,High Energy Physics - Phenomenology,High Energy Physics - Theory},
  file = {/home/hermasl/Zotero/storage/CKZMHWWK/D'Amico et al. - 2021 - Limits on $w$CDM from the EFTofLSS with the PyBird code.pdf;/home/hermasl/Zotero/storage/CS8XJRUT/2003.html}
}

@article{dasCosmicMicrowaveBackground2020,
  title = {Cosmic {{Microwave Background Anisotropy}} Numerical Solution ({{CMBAns}}) {{I}}: {{An}} Introduction to \${{C}}\_l\$ Calculation},
  shorttitle = {Cosmic {{Microwave Background Anisotropy}} Numerical Solution ({{CMBAns}}) {{I}}},
  author = {Das, Santanu and Phan, Anh},
  year = 2020,
  month = may,
  journal = {JCAP},
  volume = {2020},
  number = {05},
  eprint = {1910.00725},
  primaryclass = {astro-ph},
  pages = {006--006},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2020/05/006},
  url = {http://arxiv.org/abs/1910.00725},
  urldate = {2024-12-18},
  abstract = {Cosmological Boltzmann codes are often used by researchers for calculating the CMB angular power spectra from different theoretical models, for cosmological parameter estimation, etc. Therefore, the accuracy of a Boltzmann code is of utmost importance. Different Markov Chain Monte Carlo based parameter estimation algorithms typically require 103 -104 iterations of Boltzmann code. This makes the time complexity of such codes another critical factor. In the last two decades, several Boltzmann packages, such as CMBFAST, CAMB, CMBEasy, CLASS etc., have been developed. In this paper, we present a new cosmological Boltzmann code, CMBAns, that can be used for accurate calculation of the CMB power spectrum. At present, CMBAns is developed for a flat background matrix. It is mostly written in the C language. However, we borrowed the concept of class from C++. This gives researchers the flexibility to develop their own independent package based on CMBAns, without an in-depth understanding of the source code. We also develop multiple stand-alone facilities which can be directly compiled and run on a given parameter set. In this paper, we discuss all the mathematical formulation, approximation schemes, integration methods etc., that are used in CMBAns. The package will be made available through github for public use in the near future.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/PEA7Y5HM/Das and Phan - 2020 - Cosmic Microwave Background Anisotropy numerical s.pdf}
}

@article{desicollaborationDESIExperimentPart2016,
  title = {The {{DESI Experiment Part I}}: {{Science}},{{Targeting}}, and {{Survey Design}}},
  shorttitle = {The {{DESI Experiment Part I}}},
  author = {{DESI Collaboration}},
  year = 2016,
  month = dec,
  journal = {ArXiv},
  number = {arXiv:1611.00036},
  eprint = {1611.00036},
  primaryclass = {astro-ph},
  doi = {10.48550/arXiv.1611.00036},
  urldate = {2025-07-04},
  abstract = {DESI (Dark Energy Spectroscopic Instrument) is a Stage IV ground-based dark energy experiment that will study baryon acoustic oscillations (BAO) and the growth of structure through redshift-space distortions with a wide-area galaxy and quasar redshift survey. To trace the underlying dark matter distribution, spectroscopic targets will be selected in four classes from imaging data. We will measure luminous red galaxies up to z = 1.0. To probe the Universe out to even higher redshift, DESI will target bright [O II] emission line galaxies up to z = 1.7. Quasars will be targeted both as direct tracers of the underlying dark matter distribution and, at higher redshifts (2.1 {$<$} z {$<$} 3.5), for the Ly-{$\alpha$} forest absorption features in their spectra, which will be used to trace the distribution of neutral hydrogen. When moonlight prevents efficient observations of the faint targets of the baseline survey, DESI will conduct a magnitude-limited Bright Galaxy Survey comprising approximately 10 million galaxies with a median z {$\approx$} 0.2. In total, more than 30 million galaxy and quasar redshifts will be obtained to measure the BAO feature and determine the matter power spectrum, including redshift space distortions.},
  archiveprefix = {arXiv},
  langid = {english}
}

@article{dewdneySquareKilometreArray2009,
  title = {The {{Square Kilometre Array}}},
  author = {Dewdney, Peter E. and Hall, Peter J. and Schilizzi, Richard T. and Lazio, T. Joseph L. W.},
  year = 2009,
  month = aug,
  journal = {Proceedings of the IEEE},
  volume = {97},
  number = {8},
  pages = {1482--1496},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2009.2021005},
  url = {https://ieeexplore.ieee.org/document/5136190/},
  urldate = {2025-07-04},
  abstract = {The Square Kilometre Array (SKA) will be an ultrasensitive radio telescope, built to further the understanding of the most important phenomena in the Universe, including some pertaining to the birth and eventual death of the Universe itself. Over the next few years, the SKA will make the transition from an early formative to a well-defined design. This paper outlines how the scientific challenges are translated into technical challenges, how the application of recent technology offers the potential of affordably meeting these challenges, and how the choices of technology will ultimately be made. The SKA will be an array of coherently connected antennas spread over an area about 3000 km in extent, with an aggregate antenna collecting area of up to 106 m 2 at centimeter and meter wavelengths. A key scientific requirement is the ability to carry out sensitive observations of the sky over large areas (surveys). The ldquosurvey speedrdquo of the SKA will be enabled by the application of the most up-to-date signal-processing technology available. The SKA science impact will be widely felt in astroparticle physics and cosmology, fundamental physics, galactic and extragalactic astronomy, solar system science, and astrobiology.},
  keywords = {Africa,Aperture synthesis,Apertures,digital correlator,digital data transmission,digital signal processing,Electromagnetic spectrum,Fourier imaging,History,low-noise amplifier,Observatories,Physics,Probes,radio astronomy,Radio astronomy,radio telescope,Space technology,Telescopes},
  file = {/home/hermasl/Zotero/storage/PH623F98/The Square Kilometre Array (Dewdney+, 2009).pdf}
}

@article{doranCMBEASYObjectOriented2005,
  title = {{{CMBEASY}}:: An {{Object Oriented Code}} for the {{Cosmic Microwave Background}}},
  shorttitle = {{{CMBEASY}}},
  author = {Doran, Michael},
  year = 2005,
  month = oct,
  journal = {JCAP},
  volume = {10},
  eprint = {astro-ph/0302138},
  pages = {011--011},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2005/10/011},
  url = {http://arxiv.org/abs/astro-ph/0302138},
  urldate = {2025-09-24},
  abstract = {We have ported the cmbfast package to the C++ programming language to produce cmbeasy, an object oriented code for the cosmic microwave background. The code is available at www.cmbeasy.org. We sketch the design of the new code, emphasizing the benefits of object orientation in cosmology, which allow for simple substitution of different cosmological models and gauges. Both gauge invariant perturbations and quintessence support has been added to the code. For ease of use, as well as for instruction, a graphical user interface is available.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics},
  file = {/home/hermasl/Zotero/storage/2TF7YT9R/Doran - 2005 - CMBEASY an Object Oriented Code for the Cosmic Microwave Background.pdf;/home/hermasl/Zotero/storage/2WNFNJU5/0302138.html}
}

@article{doranSpeedingCosmologicalBoltzmann2005,
  title = {Speeding {{Up Cosmological Boltzmann Codes}}},
  author = {Doran, Michael},
  year = 2005,
  month = jun,
  journal = {JCAP},
  volume = {06},
  eprint = {astro-ph/0503277},
  pages = {011--011},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2005/06/011},
  url = {http://arxiv.org/abs/astro-ph/0503277},
  urldate = {2025-06-25},
  abstract = {We introduce a novel strategy for cosmological Boltzmann codes leading to an increase in speed by a factor of \textbackslash sim 30 for small scale Fourier modes. We (re-)investigate the tight coupling approximation and obtain analytic formulae reaching up to the octupoles of photon intensity and polarization. This leads to accurate results reaching optimal precision, while still being simple. Damping rapid oscillations of small scale modes at later times, we simplify the integration of cosmological perturbations. We obtain analytic expressions for the photon density contrast and velocity as well as an estimate of the quadrupole from after last scattering until today. These analytic formulae hold well during re-ionization and are in fact negligible for realistic cosmological scenarios. However, they do extend the validity of our approach to models with very large optical depth to the last scattering surface.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Astrophysics},
  file = {/home/hermasl/Zotero/storage/NTR89DQA/Doran - 2005 - Speeding Up Cosmological Boltzmann Codes.pdf}
}

@article{euclidcollaborationEuclidOverviewEuclid2025,
  title = {Euclid. {{I}}. {{Overview}} of the {{Euclid}} Mission},
  author = {{Euclid Collaboration}},
  year = 2025,
  month = may,
  journal = {A\&A},
  volume = {697},
  eprint = {2405.13491},
  primaryclass = {astro-ph},
  pages = {A1},
  issn = {0004-6361, 1432-0746},
  doi = {10.1051/0004-6361/202450810},
  urldate = {2025-07-04},
  abstract = {The current standard model of cosmology successfully describes a variety of measurements, but the nature of its main ingredients, dark matter and dark energy, remains unknown. Euclid is a medium-class mission in the Cosmic Vision 2015-2025 programme of the European Space Agency (ESA) that will provide high-resolution optical imaging, as well as near-infrared imaging and spectroscopy, over about 14,000 deg\textasciicircum 2 of extragalactic sky. In addition to accurate weak lensing and clustering measurements that probe structure formation over half of the age of the Universe, its primary probes for cosmology, these exquisite data will enable a wide range of science. This paper provides a high-level overview of the mission, summarising the survey characteristics, the various data-processing steps, and data products. We also highlight the main science objectives and expected performance.},
  archiveprefix = {arXiv},
  file = {/home/hermasl/Zotero/storage/3BMMT3FI/Euclid (Euclid Collaboration, 2025).pdf}
}

@article{fjeldeTuringjlGeneralPurposeProbabilistic2025,
  title = {Turing.Jl: {{A General-Purpose Probabilistic Programming Language}}},
  shorttitle = {Turing.Jl},
  author = {Fjelde, Tor Erlend and Xu, Kai and Widmann, David and Tarek, Mohamed and Pfiffer, Cameron and Trapp, Martin and Axen, Seth D. and Sun, Xianda and Hauru, Markus and Yong, Penelope and Tebbutt, Will and Ghahramani, Zoubin and Ge, Hong},
  year = 2025,
  month = sep,
  journal = {ACM Trans. Probab. Mach. Learn.},
  volume = {1},
  number = {3},
  pages = {1--48},
  issn = {2836-8924},
  doi = {10.1145/3711897},
  url = {https://dl.acm.org/doi/10.1145/3711897},
  urldate = {2025-09-24},
  abstract = {Probabilistic programming languages (PPLs) are becoming increasingly important in many scientific disciplines, such as economics, epidemiology, and biology, to extract meaning from sources of data while accounting for one's uncertainty. The key idea of probabilistic programming is to decouple inference and model specification, thus allowing the practitioner to approach their task at hand using Bayesian inference, without requiring extensive knowledge in programming or computational statistics. At the same time, the complexity of problem settings in which PPLs are employed is steadily increasing, both in terms of project size and model complexity, calling for more flexible and efficient systems.                            In this work, we describe               Turing.jl               , a general-purpose PPL, which is designed to be flexible, efficient, and easy to use.               Turing.jl               is built on top of the Julia programming language, which is known for its high performance and ease-of-use. We describe the design of               Turing.jl               , contextualizing it within different types of users and use-cases, its key features, and how it can be used to solve a wide range of problems. We also provide a brief overview of the ecosystem around               Turing.jl               , including the different libraries and tools that can be used in conjunction with it. Finally, we provide a few examples of how               Turing.jl               can be used in practice.},
  langid = {english}
}

@article{freedmanCosmologyCrossroads2017,
  title = {Cosmology at a Crossroads},
  author = {Freedman, Wendy L.},
  year = 2017,
  month = may,
  journal = {Nat Astron},
  volume = {1},
  number = {5},
  eprint = {1706.02739},
  primaryclass = {astro-ph},
  pages = {0121},
  issn = {2397-3366},
  doi = {10.1038/s41550-017-0121},
  url = {https://arxiv.org/abs/1706.02739},
  urldate = {2025-09-24},
  archiveprefix = {arXiv},
  langid = {english}
}

@misc{gebhardt2FASTFastAccurate2017,
  title = {2-{{FAST}}: {{Fast}} and Accurate Computation of Projected Two-Point Functions},
  shorttitle = {2-{{FAST}}},
  author = {Gebhardt, Henry S. Grasshorn and Jeong, Donghui},
  year = 2017,
  month = dec,
  number = {arXiv:1709.02401},
  eprint = {1709.02401},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1709.02401},
  url = {http://arxiv.org/abs/1709.02401},
  urldate = {2024-12-04},
  abstract = {We present the 2-point function from Fast and Accurate Spherical Bessel Transformation (2-FAST) algorithm for a fast and accurate computation of integrals involving one or two spherical Bessel functions. These types of integrals occur when projecting the galaxy power spectrum \$P(k)\$ onto the configuration space, \$\textbackslash xi\_\textbackslash ell\textasciicircum\textbackslash nu(r)\$, or spherical harmonic space, \$C\_\textbackslash ell(\textbackslash chi,\textbackslash chi')\$. First, we employ the FFTlog transformation of the power spectrum to divide the calculation into \$P(k)\$-dependent coefficients and \$P(k)\$-independent integrations of basis functions multiplied by spherical Bessel functions. We find analytical expressions for the latter integrals in terms of special functions, for which recursion provides a fast and accurate evaluation. The algorithm, therefore, circumvents direct integration of highly oscillating spherical Bessel functions.},
  archiveprefix = {arXiv},
  file = {/home/hermasl/Zotero/storage/T2QJDH6G/2-FAST (Gebhardt+, 2017).pdf;/home/hermasl/Zotero/storage/F3GVSQAS/1709.html}
}

@article{gowdaHighperformanceSymbolicnumericsMultiple2022,
  title = {High-Performance Symbolic-Numerics via Multiple Dispatch},
  author = {Gowda, Shashi and Ma, Yingbo and Cheli, Alessandro and Gwozdz, Maja and Shah, Viral B. and Edelman, Alan and Rackauckas, Christopher},
  year = 2022,
  month = feb,
  journal = {ArXiv},
  eprint = {2105.03949},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2105.03949},
  url = {http://arxiv.org/abs/2105.03949},
  urldate = {2025-06-23},
  abstract = {As mathematical computing becomes more democratized in high-level languages, high-performance symbolic-numeric systems are necessary for domain scientists and engineers to get the best performance out of their machine without deep knowledge of code optimization. Naturally, users need different term types either to have different algebraic properties for them, or to use efficient data structures. To this end, we developed Symbolics.jl, an extendable symbolic system which uses dynamic multiple dispatch to change behavior depending on the domain needs. In this work we detail an underlying abstract term interface which allows for speed without sacrificing generality. We show that by formalizing a generic API on actions independent of implementation, we can retroactively add optimized data structures to our system without changing the pre-existing term rewriters. We showcase how this can be used to optimize term construction and give a 113x acceleration on general symbolic transformations. Further, we show that such a generic API allows for complementary term-rewriting implementations. We demonstrate the ability to swap between classical term-rewriting simplifiers and e-graph-based term-rewriting simplifiers. We showcase an e-graph ruleset which minimizes the number of CPU cycles during expression evaluation, and demonstrate how it simplifies a real-world reaction-network simulation to halve the runtime. Additionally, we show a reaction-diffusion partial differential equation solver which is able to be automatically converted into symbolic expressions via multiple dispatch tracing, which is subsequently accelerated and parallelized to give a 157x simulation speedup. Together, this presents Symbolics.jl as a next-generation symbolic-numeric computing environment geared towards modeling and simulation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Mathematical Software,Computer Science - Programming Languages,Computer Science - Symbolic Computation},
  file = {/home/hermasl/Zotero/storage/PF4PPAC8/High-performance symbolic-numerics via multiple dispatch (Gowda+, 2022).pdf;/home/hermasl/Zotero/storage/Y8NW9H3U/2105.html}
}

@book{griewankEvaluatingDerivatives2008,
  title = {Evaluating {{Derivatives}}},
  author = {Griewank, Andreas and Walther, Andrea},
  year = 2008,
  month = jan,
  series = {Other {{Titles}} in {{Applied Mathematics}}},
  edition = {2},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9780898717761},
  url = {https://epubs.siam.org/doi/abs/10.1137/1.9780898717761},
  urldate = {2025-07-04},
  isbn = {978-0-89871-659-7},
  keywords = {adjoints,Algorithmic Differentiation,Chain rule,Computation of Derivatives,computational graph}
}

@article{hahnDISCODJDifferentiableEinsteinBoltzmann2024,
  title = {{{DISCO-DJ I}}: A Differentiable {{Einstein-Boltzmann}} Solver for Cosmology},
  shorttitle = {{{DISCO-DJ I}}},
  author = {Hahn, Oliver and List, Florian and Porqueres, Natalia},
  year = 2024,
  month = jun,
  journal = {JCAP},
  volume = {06},
  eprint = {2311.03291},
  primaryclass = {astro-ph},
  pages = {063},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2024/06/063},
  url = {http://arxiv.org/abs/2311.03291},
  urldate = {2025-09-24},
  abstract = {We present the Einstein-Boltzmann module of the DISCO-DJ (DIfferentiable Simulations for COsmology - Done with JAX) software package. This module implements a fully differentiable solver for the linearised cosmological Einstein-Boltzmann equations in the JAX framework, and allows computing Jacobian matrices of all solver output with respect to all input parameters using automatic differentiation. This implies that along with the solution for a given set of parameters, the tangent hyperplane in parameter space is known as well, which is a key ingredient for cosmological inference and forecasting problems as well as for many other applications. We discuss our implementation and demonstrate that our solver agrees at the per-mille level with the existing non-differentiable solvers CAMB and CLASS, including massive neutrinos and a dark energy fluid with parameterised equation of state. We illustrate the dependence of various summary statistics in large-scale structure cosmology on model parameters using the differentiable solver, and finally demonstrate how it can be easily used for Fisher forecasting, with a forecast for Euclid as an example. Since the implementation is significantly shorter and more modular than existing solvers, we believe it will be more straightforward to extend our solver to include additional physics, such as additional dark energy and dark matter models, modified gravity, or other non-standard physics in the future.},
  archiveprefix = {arXiv},
  file = {/home/hermasl/Zotero/storage/XRR6GGNE/Hahn et al. - 2024 - DISCO-DJ I a differentiable Einstein-Boltzmann solver for cosmology.pdf;/home/hermasl/Zotero/storage/Y6I7LDNN/2311.html}
}

@article{hastingsMonteCarloSampling1970,
  title = {Monte {{Carlo}} Sampling Methods Using {{Markov}} Chains and Their Applications},
  author = {Hastings, W. K.},
  year = 1970,
  month = apr,
  journal = {Biometrika},
  volume = {57},
  number = {1},
  pages = {97--109},
  issn = {0006-3444},
  doi = {10.1093/biomet/57.1.97},
  url = {https://doi.org/10.1093/biomet/57.1.97},
  urldate = {2025-08-13},
  abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
  file = {/home/hermasl/Zotero/storage/ZMJCEMRG/Monte Carlo sampling methods using Markov chains and their applications (Hastings, 1970).pdf;/home/hermasl/Zotero/storage/CDY2RH4V/284580.html}
}

@article{hoffmannHitchhikersGuideAutomatic2016,
  title = {A {{Hitchhiker}}'s {{Guide}} to {{Automatic Differentiation}}},
  author = {Hoffmann, Philipp H. W.},
  year = 2016,
  month = jul,
  journal = {Numer Algor},
  volume = {72},
  number = {3},
  eprint = {1411.0583},
  primaryclass = {math},
  pages = {775--811},
  issn = {1017-1398, 1572-9265},
  doi = {10.1007/s11075-015-0067-6},
  url = {http://arxiv.org/abs/1411.0583},
  urldate = {2025-06-25},
  abstract = {This article provides an overview of some of the mathematical principles of Automatic Differentiation (AD). In particular, we summarise different descriptions of the Forward Mode of AD, like the matrix-vector product based approach, the idea of lifting functions to the algebra of dual numbers, the method of Taylor series expansion on dual numbers and the application of the push-forward operator, and explain why they all reduce to the same actual chain of computations. We further give a short mathematical description of some methods of higher-order Forward AD and, at the end of this paper, briefly describe the Reverse Mode of Automatic Differentiation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Numerical Analysis,Mathematics - Numerical Analysis},
  file = {/home/hermasl/Zotero/storage/H4DH49N6/A Hitchhiker's Guide to Automatic Differentiation (Hoffmann, 2016).pdf;/home/hermasl/Zotero/storage/YZBFCQ2M/1411.html}
}

@article{hoffmanNoUTurnSamplerAdaptively2011,
  title = {The {{No-U-Turn Sampler}}: {{Adaptively Setting Path Lengths}} in {{Hamiltonian Monte Carlo}}},
  shorttitle = {The {{No-U-Turn Sampler}}},
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  year = 2011,
  month = nov,
  journal = {ArXiv},
  eprint = {1111.4246},
  primaryclass = {stat},
  doi = {10.48550/arXiv.1111.4246},
  url = {http://arxiv.org/abs/1111.4246},
  urldate = {2025-07-04},
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size \textbraceleft\textbackslash epsilon\textbraceright{} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \textbraceleft\textbackslash epsilon\textbraceright{} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation},
  file = {/home/hermasl/Zotero/storage/5GKHNUSW/The No-U-Turn Sampler (Hoffman+, 2011).pdf;/home/hermasl/Zotero/storage/4VD89KLQ/1111.html}
}

@article{howlettCMBPowerSpectrum2012,
  title = {{{CMB}} Power Spectrum Parameter Degeneracies in the Era of Precision Cosmology},
  author = {Howlett, Cullan and Lewis, Antony and Hall, Alex and Challinor, Anthony},
  year = 2012,
  month = apr,
  journal = {J. Cosmol. Astropart. Phys.},
  volume = {2012},
  number = {04},
  eprint = {1201.3654},
  primaryclass = {astro-ph},
  pages = {027--027},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2012/04/027},
  url = {http://arxiv.org/abs/1201.3654},
  urldate = {2025-12-09},
  abstract = {Cosmological parameter constraints from the CMB power spectra alone suffer several well-known degeneracies. These degeneracies can be broken by numerical artefacts and also a variety of physical effects that become quantitatively important with high-accuracy data e.g. from the Planck satellite. We study degeneracies in models with flat and non-flat spatial sections, non-trivial dark energy and massive neutrinos, and investigate the importance of various physical degeneracy-breaking effects. We test the CAMB power spectrum code for numerical accuracy, and demonstrate that the numerical calculations are accurate enough for degeneracies to be broken mainly by true physical effects (the integrated Sachs-Wolfe effect, CMB lensing and geometrical and other effects through recombination) rather than numerical artefacts. We quantify the impact of CMB lensing on the power spectra, which inevitably provides degeneracy-breaking information even without using information in the non-Gaussianity. Finally we check the numerical accuracy of sample-based parameter constraints using CAMB and CosmoMC. In an appendix we document recent changes to CAMB's numerical treatment of massive neutrino perturbations, which are tested along with other recent improvements by our degeneracy exploration results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {/home/hermasl/Zotero/storage/ACY9F5KJ/Howlett et al. - 2012 - CMB power spectrum parameter degeneracies in the era of precision cosmology.pdf}
}

@article{huCMBAnisotropiesTotal1997,
  title = {{{CMB Anisotropies}}: {{Total Angular Momentum Method}}},
  shorttitle = {{{CMB Anisotropies}}},
  author = {Hu, Wayne and White, Martin},
  year = 1997,
  month = jul,
  journal = {Phys. Rev. D},
  volume = {56},
  number = {2},
  eprint = {astro-ph/9702170},
  pages = {596--615},
  issn = {0556-2821, 1089-4918},
  doi = {10.1103/PhysRevD.56.596},
  url = {http://arxiv.org/abs/astro-ph/9702170},
  urldate = {2025-02-10},
  abstract = {A total angular momentum representation simplifies the radiation transport problem for temperature and polarization anisotropy in the CMB. Scattering terms couple only the quadrupole moments of the distributions and each moment corresponds directly to the observable angular pattern on the sky. We develop and employ these techniques to study the general properties of anisotropy generation from scalar, vector and tensor perturbations to the metric and the matter, both in the cosmological fluids and from any seed perturbations (e.g. defects) that may be present. The simpler, more transparent form and derivation of the Boltzmann equations brings out the geometric and model-independent aspects of temperature and polarization anisotropy formation. Large angle scalar polarization provides a robust means to distinguish between isocurvature and adiabatic models for structure formation in principle. Vector modes have the unique property that the CMB polarization is dominated by magnetic type parity at small angles (a factor of 6 in power compared with 0 for the scalars and 8/13 for the tensors) and hence potentially distinguishable independent of the model for the seed. The tensor modes produce a different sign from the scalars and vectors for the temperature-polarization correlations at large angles. We explore conditions under which one perturbation type may dominate over the others including a detailed treatment of the photon-baryon fluid before recombination.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Astrophysics},
  file = {/home/hermasl/Zotero/storage/UU2FUK4B/Hu and White - 1997 - CMB Anisotropies Total Angular Momentum Method.pdf}
}

@misc{huynhOverviewSquareKilometre2013,
  title = {An {{Overview}} of the {{Square Kilometre Array}}},
  author = {Huynh, Minh and Lazio, Joseph},
  year = 2013,
  month = nov,
  number = {arXiv:1311.4288},
  eprint = {1311.4288},
  primaryclass = {astro-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1311.4288},
  url = {http://arxiv.org/abs/1311.4288},
  urldate = {2025-07-04},
  abstract = {The Square Kilometre Array (SKA) will be the premier instrument to study radiation at centimetre and metre wavelengths from the cosmos, and in particular hydrogen, the most abundant element in the universe. The SKA will probe the dawn of galaxy formation as well as allow advances in many other areas of astronomy, such as fundamental physics, astrobiology and cosmology. Phase 1, which will be about 10\% of the full SKA collecting area, will be built in Australia and South Africa. This paper describes the key science drivers of the SKA, provides an update on recent SKA Organisation activities and summarises the baseline design for Phase 1.},
  archiveprefix = {arXiv},
  file = {/home/hermasl/Zotero/storage/3SCTQ9DT/An Overview of the Square Kilometre Array (Huynh+, 2013).pdf;/home/hermasl/Zotero/storage/JI85PG7T/1311.html}
}

@misc{jorgensenFamilyESDIRKIntegration2018,
  title = {A {{Family}} of {{ESDIRK Integration Methods}}},
  author = {J{\o}rgensen, John Bagterp and Kristensen, Morten Rode and Thomsen, Per Grove},
  year = 2018,
  month = mar,
  number = {arXiv:1803.01613},
  eprint = {1803.01613},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.01613},
  url = {http://arxiv.org/abs/1803.01613},
  urldate = {2025-12-16},
  abstract = {In this paper we derive and analyze the properties of explicit singly diagonal implicit Runge-Kutta (ESDIRK) integration methods. We discuss the principles for construction of Runge-Kutta methods with embedded methods of different order for error estimation and continuous extensions for discrete event location. These principles are used to derive a family of ESDIRK integration methods with error estimators and continuous-extensions. The orders of the advancing method (and error estimator) are 1(2), 2(3) and 3(4), respectively. These methods are suitable for obtaining low to medium accuracy solutions of systems of ordinary differential equations as well as index-1 differential algebraic equations. The continuous extensions facilitates solution of hybrid systems with discrete-events. Other ESDIRK methods due to Kv\ae rn\o{} are equipped with continuousextensions as well to make them applicable to hybrid systems with discrete events.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Numerical Analysis},
  file = {/home/hermasl/Zotero/storage/9RYWJRQQ/Jørgensen et al. - 2018 - A Family of ESDIRK Integration Methods.pdf}
}

@article{kennedyAdditiveRungeKutta2003,
  title = {Additive {{Runge}}--{{Kutta}} Schemes for Convection--Diffusion--Reaction Equations},
  author = {Kennedy, Christopher A. and Carpenter, Mark H.},
  year = 2003,
  month = jan,
  journal = {Applied Numerical Mathematics},
  volume = {44},
  number = {1-2},
  pages = {139--181},
  issn = {01689274},
  doi = {10.1016/S0168-9274(02)00138-1},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0168927402001381},
  urldate = {2025-07-04},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/ZRYN8BP7/Kennedy and Carpenter - 2003 - Additive Runge–Kutta schemes for convection–diffus.pdf}
}

@misc{langRosenbrockWannerMethodsConstruction2020,
  title = {Rosenbrock-{{Wanner Methods}}: {{Construction}} and {{Mission}}},
  shorttitle = {Rosenbrock-{{Wanner Methods}}},
  author = {Lang, Jens},
  year = 2020,
  month = feb,
  number = {arXiv:2002.12028},
  eprint = {2002.12028},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.12028},
  url = {http://arxiv.org/abs/2002.12028},
  urldate = {2025-12-16},
  abstract = {This paper is concerned with the history of Rosenbrock-Wanner methods first initiated by Rosenbrock in 1963. His original ideas are highlighted and the main developments over 56 years are reviewed.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - History and Overview},
  file = {/home/hermasl/Zotero/storage/ZMGQ8NIV/Lang - 2020 - Rosenbrock-Wanner Methods Construction and Mission.pdf}
}

@misc{laureijsEuclidDefinitionStudy2011a,
  title = {Euclid {{Definition Study Report}}},
  author = {Laureijs, R. and Amiaux, J. and Arduini, S. and Augu{\`e}res, J.-L. and Brinchmann, J. and Cole, R. and Cropper, M. and Dabin, C. and Duvet, L. and Ealet, A. and Garilli, B. and Gondoin, P. and Guzzo, L. and Hoar, J. and Hoekstra, H. and Holmes, R. and Kitching, T. and Maciaszek, T. and Mellier, Y. and Pasian, F. and Percival, W. and Rhodes, J. and Criado, G. Saavedra and Sauvage, M. and Scaramella, R. and Valenziano, L. and Warren, S. and Bender, R. and Castander, F. and Cimatti, A. and F{\`e}vre, O. Le and {Kurki-Suonio}, H. and Levi, M. and Lilje, P. and Meylan, G. and Nichol, R. and Pedersen, K. and Popa, V. and Lopez, R. Rebolo and Rix, H.-W. and Rottgering, H. and Zeilinger, W. and Grupp, F. and Hudelot, P. and Massey, R. and Meneghetti, M. and Miller, L. and Paltani, S. and {Paulin-Henriksson}, S. and Pires, S. and Saxton, C. and Schrabback, T. and Seidel, G. and Walsh, J. and Aghanim, N. and Amendola, L. and Bartlett, J. and Baccigalupi, C. and Beaulieu, J.-P. and Benabed, K. and Cuby, J.-G. and Elbaz, D. and Fosalba, P. and Gavazzi, G. and Helmi, A. and Hook, I. and Irwin, M. and Kneib, J.-P. and Kunz, M. and Mannucci, F. and Moscardini, L. and Tao, C. and Teyssier, R. and Weller, J. and Zamorani, G. and Osorio, M. R. Zapatero and Boulade, O. and Foumond, J. J. and Giorgio, A. Di and Guttridge, P. and James, A. and Kemp, M. and Martignac, J. and Spencer, A. and Walton, D. and Bl{\"u}mchen, T. and Bonoli, C. and Bortoletto, F. and Cerna, C. and Corcione, L. and Fabron, C. and Jahnke, K. and Ligori, S. and Madrid, F. and Martin, L. and Morgante, G. and Pamplona, T. and Prieto, E. and Riva, M. and Toledo, R. and Trifoglio, M. and Zerbi, F. and Abdalla, F. and Douspis, M. and Grenet, C. and Borgani, S. and Bouwens, R. and Courbin, F. and Delouis, J.-M. and Dubath, P. and Fontana, A. and Frailis, M. and Grazian, A. and Koppenh{\"o}fer, J. and Mansutti, O. and Melchior, M. and Mignoli, M. and Mohr, J. and Neissner, C. and Noddle, K. and Poncet, M. and Scodeggio, M. and Serrano, S. and Shane, N. and Starck, J.-L. and Surace, C. and Taylor, A. and {Verdoes-Kleijn}, G. and Vuerli, C. and Williams, O. R. and Zacchei, A. and Altieri, B. and Sanz, I. Escudero and Kohley, R. and Oosterbroek, T. and Astier, P. and Bacon, D. and Bardelli, S. and Baugh, C. and Bellagamba, F. and Benoist, C. and Bianchi, D. and Biviano, A. and Branchini, E. and Carbone, C. and Cardone, V. and Clements, D. and Colombi, S. and Conselice, C. and Cresci, G. and Deacon, N. and Dunlop, J. and Fedeli, C. and Fontanot, F. and Franzetti, P. and Giocoli, C. and {Garcia-Bellido}, J. and Gow, J. and Heavens, A. and Hewett, P. and Heymans, C. and Holland, A. and Huang, Z. and Ilbert, O. and Joachimi, B. and Jennins, E. and Kerins, E. and Kiessling, A. and Kirk, D. and Kotak, R. and Krause, O. and Lahav, O. and van Leeuwen, F. and Lesgourgues, J. and Lombardi, M. and Magliocchetti, M. and Maguire, K. and Majerotto, E. and Maoli, R. and Marulli, F. and Maurogordato, S. and McCracken, H. and McLure, R. and Melchiorri, A. and Merson, A. and Moresco, M. and Nonino, M. and Norberg, P. and Peacock, J. and Pello, R. and Penny, M. and Pettorino, V. and Porto, C. Di and Pozzetti, L. and Quercellini, C. and Radovich, M. and Rassat, A. and Roche, N. and Ronayette, S. and Rossetti, E. and Sartoris, B. and Schneider, P. and Semboloni, E. and Serjeant, S. and Simpson, F. and Skordis, C. and Smadja, G. and Smartt, S. and Spano, P. and Spiro, S. and Sullivan, M. and Tilquin, A. and Trotta, R. and Verde, L. and Wang, Y. and Williger, G. and Zhao, G. and Zoubian, J. and Zucca, E.},
  year = 2011,
  month = oct,
  number = {arXiv:1110.3193},
  eprint = {1110.3193},
  primaryclass = {astro-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1110.3193},
  url = {http://arxiv.org/abs/1110.3193},
  urldate = {2025-06-25},
  abstract = {Euclid is a space-based survey mission from the European Space Agency designed to understand the origin of the Universe's accelerating expansion. It will use cosmological probes to investigate the nature of dark energy, dark matter and gravity by tracking their observational signatures on the geometry of the universe and on the cosmic history of structure formation. The mission is optimised for two independent primary cosmological probes: Weak gravitational Lensing (WL) and Baryonic Acoustic Oscillations (BAO). The Euclid payload consists of a 1.2 m Korsch telescope designed to provide a large field of view. It carries two instruments with a common field-of-view of \textasciitilde 0.54 deg2: the visual imager (VIS) and the near infrared instrument (NISP) which contains a slitless spectrometer and a three bands photometer. The Euclid wide survey will cover 15,000 deg2 of the extragalactic sky and is complemented by two 20 deg2 deep fields. For WL, Euclid measures the shapes of 30-40 resolved galaxies per arcmin2 in one broad visible R+I+Z band (550-920 nm). The photometric redshifts for these galaxies reach a precision of dz/(1+z) {$<$} 0.05. They are derived from three additional Euclid NIR bands (Y, J, H in the range 0.92-2.0 micron), complemented by ground based photometry in visible bands derived from public data or through engaged collaborations. The BAO are determined from a spectroscopic survey with a redshift accuracy dz/(1+z) =0.001. The slitless spectrometer, with spectral resolution \textasciitilde 250, predominantly detects Ha emission line galaxies. Euclid is a Medium Class mission of the ESA Cosmic Vision 2015-2025 programme, with a foreseen launch date in 2019. This report (also known as the Euclid Red Book) describes the outcome of the Phase A study.},
  archiveprefix = {arXiv},
  file = {/home/hermasl/Zotero/storage/EWFJTH8M/Euclid Definition Study Report (Laureijs+, 2011).pdf;/home/hermasl/Zotero/storage/T5HBSU3P/1110.html}
}

@misc{leeEfficientAnalyticApproximation2025,
  title = {Efficient Analytic Approximation for Small-Scale Non-Cold Relic Perturbations},
  author = {Lee, Nanoom and {Ali-Ha{\"i}moud}, Yacine and Kamionkowski, Marc},
  year = 2025,
  month = oct,
  number = {arXiv:2510.20821},
  eprint = {2510.20821},
  primaryclass = {astro-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.20821},
  url = {http://arxiv.org/abs/2510.20821},
  urldate = {2025-10-27},
  abstract = {We develop a highly accurate analytic approximation for small-scale non-cold relic perturbations by solving the collisionless Boltzmann equation in the quasi-stationary regime. The approximation is implemented in CLASSIER (CLASS Integral Equation Revision), a modified version of the Boltzmann solver CLASS that replaces the traditional truncated Boltzmann hierarchy of non cold relic multipoles with a small set of integral equations solved iteratively. Applying it to massive neutrinos yields a factor-of-two reduction in total runtime relative to CLASSIER without the approximation. Compared to standard CLASS runs (with \$\textbackslash ell\_\textbraceleft\textbackslash rm max\textbraceright\textasciicircum\textbraceleft\textbackslash rm NCDM\textbraceright =40\$ and no late-time massive neutrino fluid approximation) under the same precision setting, CLASSIER with this approximation is faster by a factor of 3-6. The approximation faithfully reproduces the late-time behavior of massive neutrino perturbations and preserves sub-\$0.1\textbackslash\%\$ accuracy in the matter power spectrum today up to comoving wavenumber \$k=100\textbackslash,\textbraceleft\textbackslash rm Mpc\textbraceright\textasciicircum\textbraceleft -1\textbraceright\$. With this approximation, massive-neutrino perturbations are no longer the computational bottleneck on small scales for linear-theory predictions. The approach can be readily extendable to non-standard dark-matter models, and offers prospects for further efficiency gains in high-precision cosmological analyses.},
  archiveprefix = {arXiv},
  file = {/home/hermasl/Zotero/storage/AP7QKUNR/Lee et al. - 2025 - Efficient analytic approximation for small-scale non-cold relic perturbations.pdf;/home/hermasl/Zotero/storage/JM24SAGL/2510.html}
}

@article{lesgourguesCosmicLinearAnisotropy2011,
  title = {The {{Cosmic Linear Anisotropy Solving System}} ({{CLASS}}) {{I}}: {{Overview}}},
  shorttitle = {The {{Cosmic Linear Anisotropy Solving System}} ({{CLASS}}) {{I}}},
  author = {Lesgourgues, Julien},
  year = 2011,
  month = may,
  journal = {ArXiv},
  eprint = {1104.2932},
  primaryclass = {astro-ph},
  doi = {10.48550/arXiv.1104.2932},
  url = {http://arxiv.org/abs/1104.2932},
  urldate = {2024-12-02},
  abstract = {The Cosmic Linear Anisotropy Solving System (CLASS) is a new accurate Boltzmann code, designed to offer a more user-friendly and flexible coding environment to cosmologists. CLASS is very structured, easy to modify, and offers a rigorous way to control the accuracy of output quantities. It is also incidentally a bit faster than other codes. In this overview, we present the general principles of CLASS and its basic structure. We insist on the friendliness and flexibility aspects, while accuracy, physical approximations and performances are discussed in a series of companion papers.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/NXTW52RL/Lesgourgues - 2011 - The Cosmic Linear Anisotropy Solving System (CLASS.pdf}
}

@article{lesgourguesCosmicLinearAnisotropy2011a,
  title = {The {{Cosmic Linear Anisotropy Solving System}} ({{CLASS}}) {{IV}}: {{Efficient}} Implementation of Non-Cold Relics},
  shorttitle = {The {{Cosmic Linear Anisotropy Solving System}} ({{CLASS}}) {{IV}}},
  author = {Lesgourgues, Julien and Tram, Thomas},
  year = 2011,
  month = sep,
  journal = {JCAP},
  volume = {09},
  eprint = {1104.2935},
  primaryclass = {astro-ph},
  pages = {032--032},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2011/09/032},
  url = {http://arxiv.org/abs/1104.2935},
  urldate = {2025-08-11},
  abstract = {We present a new flexible, fast and accurate way to implement massive neutrinos, warm dark matter and any other non-cold dark matter relics in Boltzmann codes. For whatever analytical or numerical form of the phase-space distribution function, the optimal sampling in momentum space compatible with a given level of accuracy is automatically found by comparing quadrature methods. The perturbation integration is made even faster by switching to an approximate viscous fluid description inside the Hubble radius, which differs from previous approximations discussed in the literature. When adding one massive neutrino to the minimal cosmological model, CLASS becomes just 1.5 times slower, instead of about 5 times in other codes (for fixed accuracy requirements). We illustrate the flexibility of our approach by considering a few examples of standard or non-standard neutrinos, as well as warm dark matter models.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/3LPQCTTL/Lesgourgues and Tram - 2011 - The Cosmic Linear Anisotropy Solving System (CLASS.pdf}
}

@article{lesgourguesCosmicLinearAnisotropy2011c,
  title = {The {{Cosmic Linear Anisotropy Solving System}} ({{CLASS}}) {{III}}: {{Comparision}} with {{CAMB}} for {{LambdaCDM}}},
  shorttitle = {The {{Cosmic Linear Anisotropy Solving System}} ({{CLASS}}) {{III}}},
  author = {Lesgourgues, Julien},
  year = 2011,
  month = may,
  journal = {ArXiv},
  eprint = {1104.2934},
  primaryclass = {astro-ph},
  doi = {10.48550/arXiv.1104.2934},
  url = {http://arxiv.org/abs/1104.2934},
  urldate = {2025-09-12},
  abstract = {By confronting the two independent Boltzmann codes CLASS and CAMB, we establish that for concordance cosmology and for a given recombination history, lensed CMB and matter power spectra can be computed by current codes with an accuracy of 0.01\%. We list a few tiny changes in CAMB which are necessary in order to reach such a level. Using the common limit of the two codes as a set of reference spectra, we derive precision settings corresponding to fixed levels of error in the computation of a CMB likelihood. We find that for a given precision level, CLASS is about 2.5 times faster than CAMB for computing the lensed CMB spectra of a LambdaCDM model. The nature of the main improvements in CLASS (which may each contribute to these performances) is discussed in companion papers.},
  archiveprefix = {arXiv},
  file = {/home/hermasl/Zotero/storage/H5ZZL93Y/Lesgourgues - 2011 - The Cosmic Linear Anisotropy Solving System (CLASS) III Comparision with CAMB for LambdaCDM.pdf;/home/hermasl/Zotero/storage/CMZ2VLVF/1104.html}
}

@article{lesgourguesFastAccurateCMB2014,
  title = {Fast and Accurate {{CMB}} Computations in Non-Flat {{FLRW}} Universes},
  author = {Lesgourgues, Julien and Tram, Thomas},
  year = 2014,
  month = sep,
  journal = {JCAP},
  volume = {09},
  eprint = {1312.2697},
  primaryclass = {astro-ph},
  pages = {032--032},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2014/09/032},
  url = {http://arxiv.org/abs/1312.2697},
  urldate = {2024-12-16},
  abstract = {We present a new method for calculating CMB anisotropies in a non-flat Friedmann universe, relying on a very stable algorithm for the calculation of hyperspherical Bessel functions, that can be pushed to arbitrary precision levels. We also introduce a new approximation scheme which gradually takes over in the flat space limit and leads to significant reductions of the computation time.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/DGRG693Y/Lesgourgues and Tram - 2014 - Fast and accurate CMB computations in non-flat FLR.pdf}
}

@article{lewisCAMBNotes2025,
  title = {{{CAMB Notes}}},
  author = {Lewis, Antony},
  year = 2025,
  journal = {CAMB Notes},
  url = {https://cosmologist.info/notes/CAMB.pdf},
  file = {/home/hermasl/Zotero/storage/9FAVBZKU/CAMB Notes (Lewis, 2014).pdf}
}

@article{lewisEfficientComputationCMB2000a,
  title = {Efficient {{Computation}} of {{CMB}} Anisotropies in Closed {{FRW}} Models},
  author = {Lewis, Antony and Challinor, Anthony and Lasenby, Anthony},
  year = 2000,
  month = aug,
  journal = {ApJ},
  volume = {538},
  number = {2},
  eprint = {astro-ph/9911177},
  pages = {473--476},
  issn = {0004-637X, 1538-4357},
  doi = {10.1086/309179},
  url = {http://arxiv.org/abs/astro-ph/9911177},
  urldate = {2025-09-24},
  abstract = {We implement the efficient line of sight method to calculate the anisotropy and polarization of the cosmic microwave background for scalar and tensor modes in almost-Friedmann-Robertson-Walker models with positive spatial curvature. We present new results for the polarization power spectra in such models.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics},
  file = {/home/hermasl/Zotero/storage/4JJE3KBN/Lewis et al. - 2000 - Efficient Computation of CMB anisotropies in closed FRW models.pdf;/home/hermasl/Zotero/storage/D8VK75PJ/9911177.html}
}

@article{liBoltjl2023,
  title = {Bolt.Jl},
  shorttitle = {Xzackli/{{Bolt}}.Jl},
  author = {Li, Zack and Sullivan, Jamie and Millea, Marius},
  year = 2023,
  month = nov,
  doi = {10.5281/zenodo.10065126},
  url = {https://doi.org/10.5281/zenodo.10065125},
  urldate = {2025-06-20},
  abstract = {differentiable boltzmann code},
  file = {/home/hermasl/Zotero/storage/9BPNIKA8/10065126.html}
}

@article{linderExploringExpansionHistory2003,
  title = {Exploring the {{Expansion History}} of the {{Universe}}},
  author = {Linder, Eric V.},
  year = 2003,
  month = mar,
  journal = {Phys. Rev. Lett.},
  volume = {90},
  number = {9},
  eprint = {astro-ph/0208512},
  pages = {091301},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.90.091301},
  url = {http://arxiv.org/abs/astro-ph/0208512},
  urldate = {2025-07-04},
  abstract = {Exploring the recent expansion history of the universe promises insights into the cosmological model, the nature of dark energy, and potentially clues to high energy physics theories and gravitation. We examine the extent to which precision distance-redshift observations can map out the history, including the acceleration-deceleration transition, and the components and equations of state of the energy density. We consider the ability to distinguish between various dynamical scalar field models for the dark energy, as well as higher dimension and alternate gravity theories. Finally, we present a new, advantageous parametrization for the study of dark energy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Astrophysics},
  file = {/home/hermasl/Zotero/storage/S2SUW7D8/Linder - 2003 - Exploring the Expansion History of the Universe.pdf}
}

@misc{listDISCODJIIDifferentiable2025,
  title = {{{DISCO-DJ II}}: A Differentiable Particle-Mesh Code for Cosmology},
  shorttitle = {{{DISCO-DJ II}}},
  author = {List, Florian and Hahn, Oliver and Fl{\"o}ss, Thomas and Winkler, Lukas},
  year = 2025,
  month = oct,
  number = {arXiv:2510.05206},
  eprint = {2510.05206},
  primaryclass = {astro-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.05206},
  url = {http://arxiv.org/abs/2510.05206},
  urldate = {2025-11-06},
  abstract = {The mildly non-linear regime of cosmic structure formation holds much of the information that upcoming large-scale structure surveys aim to exploit, making fast and accurate predictions on these scales essential. We present the \$N\$-body module of DISCO-DJ (DIfferentiable Simulations for COsmology - Done with Jax), designed to deliver high-fidelity, GPU-accelerated, and differentiable particle-mesh simulations tailored for cosmological inference. Theory-informed time integrators such as the recently introduced BullFrog method allow for accurate predictions already with few time steps (e.g. \$6\$ steps for per-cent-level accuracy in terms of the present-day power spectrum at \$k \textbackslash approx 0.2 \textbackslash, h / \textbackslash mathrm\textbraceleft Mpc\textbraceright\$ using \$N = 512\textasciicircum 3\$ particles, which takes just a few seconds). To control discreteness effects and achieve high accuracy, the code incorporates a suite of advanced techniques, for example a custom non-uniform FFT implementation for force evaluation. Both forward- and reverse-mode differentiation are supported, with memory requirements independent of the number of time steps; in the reverse case, this is achieved through an adjoint formulation. We extensively study the effect of various numerical parameters on the accuracy. As an application of DISCO-DJ, we perform field-level inference by recovering \$\textbackslash sigma\_8\$ and the initial conditions from a noisy Gadget matter density field. Coupled with our recently introduced Einstein--Boltzmann solver, the DISCO-DJ ecosystem provides a self-consistent, fully differentiable pipeline for modelling the large-scale structure of the universe. The code is available at https://github.com/cosmo-sims/DISCO-DJ.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics},
  file = {/home/hermasl/Zotero/storage/BPBZGFJD/List et al. - 2025 - DISCO-DJ II a differentiable particle-mesh code for cosmology.pdf;/home/hermasl/Zotero/storage/D556EK3D/2510.html}
}

@article{lsstsciencecollaborationLSSTScienceBook2009,
  title = {{{LSST Science Book}}, {{Version}} 2.0},
  author = {{LSST Science Collaboration}},
  year = 2009,
  month = dec,
  journal = {ArXiv},
  number = {arXiv:0912.0201},
  eprint = {0912.0201},
  primaryclass = {astro-ph},
  doi = {10.48550/arXiv.0912.0201},
  url = {https://arxiv.org/abs/0912.0201},
  urldate = {2025-06-25},
  abstract = {A survey that can cover the sky in optical bands over wide fields to faint magnitudes with a fast cadence will enable many of the exciting science opportunities of the next decade. The Large Synoptic Survey Telescope (LSST) will have an effective aperture of 6.7 meters and an imaging camera with field of view of 9.6 deg\textasciicircum 2, and will be devoted to a ten-year imaging survey over 20,000 deg\textasciicircum 2 south of +15 deg. Each pointing will be imaged 2000 times with fifteen second exposures in six broad bands from 0.35 to 1.1 microns, to a total point-source depth of r\textasciitilde 27.5. The LSST Science Book describes the basic parameters of the LSST hardware, software, and observing plans. The book discusses educational and outreach opportunities, then goes on to describe a broad range of science that LSST will revolutionize: mapping the inner and outer Solar System, stellar populations in the Milky Way and nearby galaxies, the structure of the Milky Way disk and halo and other objects in the Local Volume, transient and variable objects both at low and high redshift, and the properties of normal and active galaxies at low and high redshift. It then turns to far-field cosmological topics, exploring properties of supernovae to z\textasciitilde 1, strong and weak lensing, the large-scale distribution of galaxies and baryon oscillations, and how these different probes may be combined to constrain cosmological models and the physics of dark energy.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Earth and Planetary Astrophysics,Astrophysics - Solar and Stellar Astrophysics}
}

@article{luDoesAcceleratingUniverse2011,
  title = {Does Accelerating Universe Indicates {{Brans-Dicke}} Theory},
  author = {Lu, Jianbo and Wang, Weiping and Xu, Lixin and Wu, Yabo},
  year = 2011,
  month = oct,
  journal = {Eur. Phys. J. Plus},
  volume = {126},
  number = {10},
  eprint = {1105.1868},
  primaryclass = {astro-ph, physics:gr-qc},
  pages = {92},
  issn = {2190-5444},
  doi = {10.1140/epjp/i2011-11092-x},
  url = {http://arxiv.org/abs/1105.1868},
  urldate = {2024-09-11},
  abstract = {The evolution of universe in Brans-Dicke (BD) theory is discussed in this paper. Considering a parameterized scenario for BD scalar field \$\textbackslash phi=\textbackslash phi\_\textbraceleft 0\textbraceright a\textasciicircum\textbraceleft\textbackslash alpha\textbraceright\$ which plays the role of gravitational "constant" \$G\$, we apply the Markov Chain Monte Carlo method to investigate a global constraints on BD theory with a self-interacting potential according to the current observational data: Union2 dataset of type supernovae Ia (SNIa), high-redshift Gamma-Ray Bursts (GRBs) data, observational Hubble data (OHD), the cluster X-ray gas mass fraction, the baryon acoustic oscillation (BAO), and the cosmic microwave background (CMB) data. It is shown that an expanded universe from deceleration to acceleration is given in this theory, and the constraint results of dimensionless matter density \$\textbackslash Omega\_\textbraceleft 0m\textbraceright\$ and parameter \$\textbackslash alpha\$ are, \$\textbackslash Omega\_\textbraceleft 0m\textbraceright =0.286\textasciicircum\textbraceleft +0.037+0.050\textbraceright\_\textbraceleft -0.039-0.047\textbraceright\$ and \$\textbackslash alpha=0.0046\textasciicircum\textbraceleft +0.0149+0.0171\textbraceright\_\textbraceleft -0.0171-0.0206\textbraceright\$ which is consistent with the result of current experiment exploration, \$\textbackslash mid\textbackslash alpha\textbackslash mid \textbackslash leq 0.132124\$. In addition, we use the geometrical diagnostic method, jerk parameter \$j\$, to distinguish the BD theory and cosmological constant model in Einstein's theory of general relativity.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/Y2FVU8KM/Lu et al. - 2011 - Does accelerating universe indicates Brans-Dicke t.pdf}
}

@misc{maComparisonAutomaticDifferentiation2021,
  title = {A {{Comparison}} of {{Automatic Differentiation}} and {{Continuous Sensitivity Analysis}} for {{Derivatives}} of {{Differential Equation Solutions}}},
  author = {Ma, Yingbo and Dixit, Vaibhav and Innes, Mike and Guo, Xingjian and Rackauckas, Christopher},
  year = 2021,
  month = jul,
  number = {arXiv:1812.01892},
  eprint = {1812.01892},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1812.01892},
  urldate = {2024-04-16},
  abstract = {Derivatives of differential equation solutions are commonly for parameter estimation, fitting neural differential equations, and as model diagnostics. However, with a litany of choices and a Cartesian product of potential methods, it can be difficult for practitioners to understand which method is likely to be the most effective on their particular application. In this manuscript we investigate the performance characteristics of Discrete Local Sensitivity Analysis implemented via Automatic Differentiation (DSAAD) against continuous adjoint sensitivity analysis. Non-stiff and stiff biological and pharmacometric models, including a PDE discretization, are used to quantify the performance of sensitivity analysis methods. Our benchmarks show that on small systems of ODEs (approximately \${$<$}100\$ parameters+ODEs), forward-mode DSAAD is more efficient than both reverse-mode and continuous forward/adjoint sensitivity analysis. The scalability of continuous adjoint methods is shown to be more efficient than discrete adjoints and forward methods after crossing this size range. These comparative studies demonstrate a trade-off between memory usage and performance in the continuous adjoint methods that should be considered when choosing the technique, while numerically unstable backsolve techniques from the machine learning literature are demonstrated as unsuitable for most scientific models. The performance of adjoint methods is shown to be heavily tied to the reverse-mode AD method, with tape-based AD methods shown to be 2 orders of magnitude slower on nonlinear partial differential equations than static AD techniques. These results also demonstrate the applicability of DSAAD to differential-algebraic equations, delay differential equations, and hybrid differential equation systems, showcasing an ease of implementation advantage for DSAAD approaches.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Numerical Analysis},
  file = {/home/hermasl/Zotero/storage/INC58B7U/A Comparison of Automatic Differentiation and Continuous Sensitivity Analysis (Ma+, 2021).pdf;/home/hermasl/Zotero/storage/VPZWH4XZ/1812.html}
}

@article{maCosmologicalPerturbationTheory1995,
  title = {Cosmological {{Perturbation Theory}} in the {{Synchronous}} and {{Conformal Newtonian Gauges}}},
  author = {Ma, Chung-Pei and Bertschinger, Edmund},
  year = 1995,
  month = dec,
  journal = {ApJ},
  volume = {455},
  eprint = {astro-ph/9506072},
  pages = {7},
  issn = {0004-637X, 1538-4357},
  doi = {10.1086/176550},
  url = {http://arxiv.org/abs/astro-ph/9506072},
  urldate = {2024-03-01},
  abstract = {This paper presents a systematic treatment of the linear theory of scalar gravitational perturbations in the synchronous gauge and the conformal Newtonian (or longitudinal) gauge. It differs from others in the literature in that we give, in both gauges, a complete discussion of all particle species that are relevant to any flat cold dark matter (CDM), hot dark matter (HDM), or CDM+HDM models (including a possible cosmological constant). The particles considered include CDM, baryons, photons, massless neutrinos, and massive neutrinos (an HDM candidate), where the CDM and baryons are treated as fluids while a detailed phase-space description is given to the photons and neutrinos. Particular care is applied to the massive neutrino component, which has been either ignored or approximated crudely in previous works. Isentropic initial conditions on super-horizon scales are derived. The coupled, linearized Boltzmann, Einstein and fluid equations that govern the evolution of the metric and density perturbations are then solved numerically in both gauges for the standard CDM model and two CDM+HDM models with neutrino mass densities \$\textbackslash onu=0.2\$ and 0.3, assuming a scale-invariant, adiabatic spectrum of primordial fluctuations. We also give the full details of the cosmic microwave background anisotropy, and present the first accurate calculations of the angular power spectra in the two CDM+HDM models including photon polarization, higher neutrino multipole moments, and helium recombination. The numerical programs for both gauges are available at http://arcturus.mit.edu/cosmics/ .},
  archiveprefix = {arXiv},
  keywords = {_tablet,Astrophysics},
  file = {/home/hermasl/Zotero/storage/EZ3S6WYM/Cosmological Perturbation Theory in the Synchronous and Conformal Newtonian (Ma+, 1995).pdf;/home/hermasl/Zotero/storage/2F6G79ZC/9506072.html}
}

@article{maModelingToolkitComposableGraph2022,
  title = {{{ModelingToolkit}}: {{A Composable Graph Transformation System For Equation-Based Modeling}}},
  shorttitle = {{{ModelingToolkit}}},
  author = {Ma, Yingbo and Gowda, Shashi and Anantharaman, Ranjan and Laughman, Chris and Shah, Viral and Rackauckas, Chris},
  year = 2022,
  month = feb,
  journal = {ArXiv},
  eprint = {2103.05244},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2103.05244},
  url = {http://arxiv.org/abs/2103.05244},
  urldate = {2025-06-23},
  abstract = {Getting good performance out of numerical equation solvers requires that the user has provided stable and efficient functions representing their model. However, users should not be trusted to write good code. In this manuscript we describe ModelingToolkit (MTK), a symbolic equation-based modeling system which allows for composable transformations to generate stable, efficient, and parallelized model implementations. MTK blurs the lines of traditional symbolic computing by acting directly on a user's numerical code. We show the ability to apply graph algorithms for automatically parallelizing and performing index reduction on code written for differential-algebraic equation (DAE) solvers, "fixing" the performance and stability of the model without requiring any changes to on the user's part. We demonstrate how composable model transformations can be combined with automated data-driven surrogate generation techniques, allowing machine learning methods to generate accelerated approximate models within an acausal modeling framework. These reduced models are shown to outperform the Dymola Modelica compiler on an HVAC model by 590x at 3\textbackslash\% error. Together, this demonstrates MTK as a system for bringing the latest research in graph transformations directly to modeling applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Mathematical Software,Computer Science - Software Engineering,Computer Science - Symbolic Computation},
  file = {/home/hermasl/Zotero/storage/94WR3E5Z/ModelingToolkit (Ma+, 2022).pdf;/home/hermasl/Zotero/storage/7WCAQTRL/2103.html}
}

@article{margossianReviewAutomaticDifferentiation2019,
  title = {A {{Review}} of Automatic Differentiation and Its Efficient Implementation},
  author = {Margossian, Charles C.},
  year = 2019,
  month = jul,
  journal = {WIREs Data Min \& Knowl},
  volume = {9},
  number = {4},
  eprint = {1811.05031},
  primaryclass = {cs},
  pages = {e1305},
  issn = {1942-4787, 1942-4795},
  doi = {10.1002/WIDM.1305},
  url = {http://arxiv.org/abs/1811.05031},
  urldate = {2025-06-25},
  abstract = {Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. Automatic differentiation is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions. The implementation of automatic differentiation however requires some care to insure efficiency. Modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management. Among these techniques are operation overloading, regionbased memory, and expression templates. There also exist several mathematical techniques which can yield high performance gains when applied to complex algorithms. For example, semi-analytical derivatives can reduce by orders of magnitude the runtime required to numerically solve and differentiate an algebraic equation. Open and practical problems include the extension of current packages to provide more specialized routines, and finding optimal methods to perform higher-order differentiation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Mathematical Software,Statistics - Computation},
  file = {/home/hermasl/Zotero/storage/8BUJWN79/Margossian - 2019 - A Review of automatic differentiation and its effi.pdf}
}

@article{meadHMcode2020ImprovedModelling2021,
  title = {{{HMcode-2020}}: {{Improved}} Modelling of Non-Linear Cosmological Power Spectra with Baryonic Feedback},
  shorttitle = {{{HMcode-2020}}},
  author = {Mead, Alexander and Brieden, Samuel and Tr{\"o}ster, Tilman and Heymans, Catherine},
  year = 2021,
  month = feb,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {502},
  number = {1},
  eprint = {2009.01858},
  primaryclass = {astro-ph},
  pages = {1401--1422},
  issn = {0035-8711, 1365-2966},
  doi = {10.1093/mnras/stab082},
  url = {http://arxiv.org/abs/2009.01858},
  urldate = {2025-11-11},
  abstract = {We present an updated version of the HMcode augmented halo model that can be used to make accurate predictions of the non-linear matter power spectrum over a wide range of cosmologies. Major improvements include modelling of BAO damping in the power spectrum and an updated treatment of massive neutrinos. We fit our model to simulated power spectra and show that we can match the results with an RMS error of 2.5 per cent across a range of cosmologies, scales \$k {$<$} 10\textbackslash,h\textbackslash mathrm\textbraceleft Mpc\textbraceright\textasciicircum\textbraceleft -1\textbraceright\$, and redshifts \$z{$<$}2\$. The error rarely exceeds 5 per cent and never exceeds 16 per cent. The worst-case errors occur at \$z\textbackslash simeq2\$, or for cosmologies with unusual dark-energy equations of state. This represents a significant improvement over previous versions of HMcode, and over other popular fitting functions, particularly for massive-neutrino cosmologies with high neutrino mass. We also present a simple halo model that can be used to model the impact of baryonic feedback on the power spectrum. This six-parameter physical model includes gas expulsion by AGN feedback and encapsulates star formation. By comparing this model to data from hydrodynamical simulations we demonstrate that the power spectrum response to feedback is matched at the \${$<$}1\$ per cent level for \$z{$<$}1\$ and \$k{$<$}20\textbackslash,h\textbackslash mathrm\textbraceleft Mpc\textbraceright\textasciicircum\textbraceleft -1\textbraceright\$. We also present a single-parameter variant of this model, parametrized in terms of feedback strength, which is only slightly less accurate. We make code available for our non-linear and baryon models at https://github.com/alexander-mead/HMcode and it is also available within CAMB and soon within CLASS.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {/home/hermasl/Zotero/storage/FYWABS87/Mead et al. - 2021 - HMcode-2020 Improved modelling of non-linear cosmological power spectra with baryonic feedback.pdf;/home/hermasl/Zotero/storage/GF7RNQYA/2009.html}
}

@article{milleaBayesianDelensingDelight2020,
  title = {Bayesian Delensing Delight: Sampling-Based Inference of the Primordial {{CMB}} and Gravitational Lensing},
  shorttitle = {Bayesian Delensing Delight},
  author = {Millea, Marius and Anderes, Ethan and Wandelt, Benjamin D.},
  year = 2020,
  month = dec,
  journal = {Phys. Rev. D},
  volume = {102},
  number = {12},
  eprint = {2002.00965},
  primaryclass = {astro-ph},
  pages = {123542},
  issn = {2470-0010, 2470-0029},
  doi = {10.1103/PhysRevD.102.123542},
  url = {http://arxiv.org/abs/2002.00965},
  urldate = {2025-11-10},
  abstract = {The search for primordial gravitational waves in the Cosmic Microwave Background (CMB) will soon be limited by our ability to remove the lensing contamination to \$B\$-mode polarization. The often-used quadratic estimator for lensing is known to be suboptimal for surveys that are currently operating and will continue to become less and less efficient as instrumental noise decreases. While foregrounds can in principle be mitigated by observing in more frequency bands, progress in delensing hinges entirely on algorithmic advances. We demonstrate here a new inference method that solves this problem by sampling the exact Bayesian posterior of any desired cosmological parameters, of the gravitational lensing potential, and of the delensed CMB maps, given lensed temperature and polarization data. We validate the method using simulated CMB data with non-white noise and masking on up to 650\textbackslash,deg\$\textasciicircum 2\$ patches of sky. A unique strength of this approach is the ability to jointly estimate cosmological parameters which control both the primordial CMB and the lensing potential, which we demonstrate here for the first time by sampling both the tensor-to-scalar ratio, \$r\$, and the amplitude of the lensing potential, \$A\_\textbackslash phi\$. The method allows us to perform the most precise check to-date of several important approximations underlying CMB-S4 \$r\$ forecasting, and we confirm these yield the correct expected uncertainty on \$r\$ to better than 10\%.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {/home/hermasl/Zotero/storage/XFUKBXXM/Millea et al. - 2020 - Bayesian delensing delight sampling-based inference of the primordial CMB and gravitational lensing.pdf}
}

@article{moserSymbolicImplementationExtensions2022,
  title = {Symbolic {{Implementation}} of {{Extensions}} of the \$\textbackslash texttt\textbraceleft{{PyCosmo}}\textbraceright\$ {{Boltzmann Solver}}},
  author = {Moser, Beatrice and Lorenz, Christiane S. and Schmitt, Uwe and Refregier, Alexandre and Fluri, Janis and Sgier, Raphael and Tarsitano, Federica and Heisenberg, Lavinia},
  year = 2022,
  month = jul,
  journal = {Astronomy and Computing},
  volume = {40},
  eprint = {2112.08395},
  primaryclass = {astro-ph},
  pages = {100603},
  issn = {22131337},
  doi = {10.1016/j.ascom.2022.100603},
  url = {http://arxiv.org/abs/2112.08395},
  urldate = {2024-03-01},
  abstract = {\$\textbackslash texttt\textbraceleft PyCosmo\textbraceright\$ is a Python-based framework for the fast computation of cosmological model predictions. One of its core features is the symbolic representation of the Einstein-Boltzmann system of equations. Efficient \$\textbackslash texttt\textbraceleft C/C++\textbraceright\$ code is generated from the \$\textbackslash texttt\textbraceleft SymPy\textbraceright\$ symbolic expressions making use of the \$\textbackslash texttt\textbraceleft sympy2c\textbraceright\$ package. This enables easy extensions of the equation system for the implementation of new cosmological models. We illustrate this with three extensions of the \$\textbackslash texttt\textbraceleft PyCosmo\textbraceright\$ Boltzmann solver to include a dark energy component with a constant equation of state, massive neutrinos and a radiation streaming approximation. We describe the \$\textbackslash texttt\textbraceleft PyCosmo\textbraceright\$ framework, highlighting new features, and the symbolic implementation of the new models. We compare the \$\textbackslash texttt\textbraceleft PyCosmo\textbraceright\$ predictions for the \$\textbackslash Lambda\$CDM model extensions with \$\textbackslash texttt\textbraceleft CLASS\textbraceright\$, both in terms of accuracy and computational speed. We find a good agreement, to better than 0.1\% when using high-precision settings and a comparable computational speed. Links to the Python Package Index (PyPI) page of the code release and to the PyCosmo Hub, an online platform where the package is installed, are available at: https://cosmology.ethz.ch/research/software-lab/PyCosmo.html.},
  archiveprefix = {arXiv},
  keywords = {_tablet},
  file = {/home/hermasl/Zotero/storage/USU25JPR/Symbolic Implementation of Extensions of the $-texttt PyCosmo $ Boltzmann Solver (Moser+, 2022).pdf;/home/hermasl/Zotero/storage/2NAMUP9K/2112.html}
}

@article{nadkarni-ghoshEinsteinBoltzmannEquationsRevisited2017,
  title = {The {{Einstein-Boltzmann}} Equations Revisited},
  author = {{Nadkarni-Ghosh}, Sharvari and Refregier, Alexandre},
  year = 2017,
  month = oct,
  journal = {MNRAS},
  volume = {471},
  number = {2},
  eprint = {1612.06697},
  primaryclass = {astro-ph, physics:gr-qc},
  pages = {2391--2430},
  issn = {0035-8711, 1365-2966},
  doi = {10.1093/mnras/stx1662},
  url = {http://arxiv.org/abs/1612.06697},
  urldate = {2024-03-01},
  abstract = {The linear Einstein-Boltzmann equations describe the evolution of perturbations in the universe and its numerical solutions play a central role in cosmology. We revisit this system of differential equations and present a detailed investigation of its mathematical properties. For this purpose, we focus on a simplified set of equations aimed at describing the broad features of the matter power spectrum. We first perform an eigenvalue analysis and study the onset of oscillations in the system signaled by the transition from real to complex eigenvalues. We then provide a stability criterion of different numerical schemes for this linear system and estimate the associated step-size. We elucidate the stiffness property of the Einstein-Boltzmann system and show how it can be characterized in terms of the eigenvalues. While the parameters of the system are time dependent making it non-autonomous, we define an adiabatic regime where the parameters vary slowly enough for the system to be quasi-autonomous. We summarize the different regimes of the system for these different criteria as function of wave number \$k\$ and scale factor \$a\$. We also provide a compendium of analytic solutions for all perturbation variables in 6 limits on the \$k\$-\$a\$ plane and express them explicitly in terms of initial conditions. These results are aimed to help the further development and testing of numerical cosmological Boltzmann solvers.},
  archiveprefix = {arXiv},
  keywords = {_tablet},
  file = {/home/hermasl/Zotero/storage/GJV88PFY/The Einstein-Boltzmann equations revisited (Nadkarni-Ghosh+, 2017).pdf;/home/hermasl/Zotero/storage/ZMIMRET7/1612.html}
}

@article{peeblesPrimevalAdiabaticPerturbation1970,
  title = {Primeval {{Adiabatic Perturbation}} in an {{Expanding Universe}}},
  author = {Peebles, P. J. E. and Yu, J. T.},
  year = 1970,
  month = dec,
  journal = {ApJ},
  volume = {162},
  pages = {815},
  issn = {0004-637X, 1538-4357},
  doi = {10.1086/150713},
  url = {http://adsabs.harvard.edu/doi/10.1086/150713},
  urldate = {2025-06-20},
  abstract = {The general qualitative behavior of linear, first-order density perturbations in a Friedmann-Lema\^itre cosmological model with radiation and matter has been known for some time in the various limiting situations. An exact quantitative calculation which traces the entire history of the density fluctuations is lacking because the usual approximations of a very short photon mean free path before plasma recombination, and a very long mean free path after, are inadequate. We present here results of the direct integration of the collision equation of the photon distribution function, which enable us to treat in detail the complicated regime of plasma recombination. Starting from an assumed initial power spectrum well before recombination, we obtain a final spectrum of density perturbations after recombination. The calculations are carried out for several general-relativity models and one scalar-tensor model. One can identify two characteristic masses in the final power spectrum: one is the mass within the Hubble radius ct at recombination, and the other results from the linear dissipation of the perturbations prior to recombination. Conceivably the first of these numbers is associated with the great rich clusters of galaxies, the second with the large galaxies. We compute also the expected residual irregularity in the radiation from the primeval fireball. If we assume that (1) the rich clusters formed from an initially adiabatic perturbation and (2) the fireball radiation has not been seriously perturbed after the epoch of recombination of the primeval plasma, then with an angular resolution of 1 minute of arc the rms fluctuation in antenna temperature should be at least bT/T = 0.00015.},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/BAIC25Y6/Peebles and Yu - 1970 - Primeval Adiabatic Perturbation in an Expanding Un.pdf}
}

@misc{peeblesStatusLambdaCDMTheory2024,
  title = {Status of the {{LambdaCDM}} Theory: Supporting Evidence and Anomalies},
  shorttitle = {Status of the {{LambdaCDM}} Theory},
  author = {Peebles, P. J. E.},
  year = 2024,
  month = may,
  number = {arXiv:2405.18307},
  eprint = {2405.18307},
  primaryclass = {astro-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.18307},
  url = {http://arxiv.org/abs/2405.18307},
  urldate = {2024-11-29},
  abstract = {The standard LambdaCDM cosmology passes demanding tests that establish it as a good approximation to reality. It is incomplete, with open questions and anomalies, but the same is true of all our physical theories. The anomalies in the standard cosmology might guide us to an even better theory. It has happened before.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/KJQFPDPF/Peebles - 2024 - Status of the LambdaCDM theory supporting evidenc.pdf}
}

@article{perivolaropoulosChallenges$L$CDMUpdate2022,
  title = {Challenges for \${{$\Lambda$}}\${{CDM}}: {{An}} Update},
  shorttitle = {Challenges for \${{$\Lambda$}}\${{CDM}}},
  author = {Perivolaropoulos, Leandros and Skara, Foteini},
  year = 2022,
  month = dec,
  journal = {New Astronomy Reviews},
  volume = {95},
  eprint = {2105.05208},
  primaryclass = {astro-ph},
  pages = {101659},
  issn = {13876473},
  doi = {10.1016/j.newar.2022.101659},
  url = {http://arxiv.org/abs/2105.05208},
  urldate = {2024-11-18},
  abstract = {A number of challenges to the standard \$\textbackslash Lambda\$CDM model have been emerging during the past few years as the accuracy of cosmological observations improves. In this review we discuss in a unified manner many existing signals in cosmological and astrophysical data that appear to be in some tension (\$2\textbackslash sigma\$ or larger) with the standard \$\textbackslash Lambda\$CDM model as specified by the Cosmological Principle, General Relativity and the Planck18 parameter values. In addition to the well-studied \$5\textbackslash sigma\$ challenge of \$\textbackslash Lambda\$CDM (the Hubble \$H\_0\$ tension) and other well known tensions (the growth tension, and the lensing amplitude \$A\_L\$ anomaly), we discuss a wide range of other less discussed less-standard signals which appear at a lower statistical significance level than the \$H\_0\$ tension some of them known as 'curiosities' in the data) which may also constitute hints towards new physics. For example such signals include cosmic dipoles (the fine structure constant \$\textbackslash alpha\$, velocity and quasar dipoles), CMB asymmetries, BAO Ly\$\textbackslash alpha\$ tension, age of the Universe issues, the Lithium problem, small scale curiosities like the core-cusp and missing satellite problems, quasars Hubble diagram, oscillating short range gravity signals etc. The goal of this pedagogical review is to collectively present the current status (2022 update) of these signals and their level of significance, with emphasis on the Hubble tension and refer to recent resources where more details can be found for each signal. We also briefly discuss theoretical approaches that can potentially explain some of these signals.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {High Energy Physics - Theory},
  file = {/home/hermasl/Zotero/storage/JS4IYBCE/Perivolaropoulos and Skara - 2022 - Challenges for $Λ$CDM An update.pdf}
}

@article{philcoxIntroductionEFTofLSS,
  title = {An {{Introduction}} to the {{EFTofLSS}}},
  author = {Philcox, Oliver H E},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/QYDW3IVL/Philcox - An Introduction to the EFTofLSS.pdf}
}

@article{pirasCosmoPowerJAXHighdimensionalBayesian2023,
  title = {{{CosmoPower-JAX}}: High-Dimensional {{Bayesian}} Inference with Differentiable Cosmological Emulators},
  shorttitle = {{{CosmoPower-JAX}}},
  author = {Piras, D. and Mancini, A. Spurio},
  year = 2023,
  month = jun,
  journal = {TheOJA},
  volume = {6},
  eprint = {2305.06347},
  primaryclass = {astro-ph},
  pages = {10.21105/astro.2305.06347},
  issn = {2565-6120},
  doi = {10.21105/astro.2305.06347},
  url = {http://arxiv.org/abs/2305.06347},
  urldate = {2024-04-03},
  abstract = {We present CosmoPower-JAX, a JAX-based implementation of the CosmoPower framework, which accelerates cosmological inference by building neural emulators of cosmological power spectra. We show how, using the automatic differentiation, batch evaluation and just-in-time compilation features of JAX, and running the inference pipeline on graphics processing units (GPUs), parameter estimation can be accelerated by orders of magnitude with advanced gradient-based sampling techniques. These can be used to efficiently explore high-dimensional parameter spaces, such as those needed for the analysis of next-generation cosmological surveys. We showcase the accuracy and computational efficiency of CosmoPower-JAX on two simulated Stage IV configurations. We first consider a single survey performing a cosmic shear analysis totalling 37 model parameters. We validate the contours derived with CosmoPower-JAX and a Hamiltonian Monte Carlo sampler against those derived with a nested sampler and without emulators, obtaining a speed-up factor of \$\textbackslash mathcal\textbraceleft O\textbraceright (10\textasciicircum 3)\$. We then consider a combination of three Stage IV surveys, each performing a joint cosmic shear and galaxy clustering (3x2pt) analysis, for a total of 157 model parameters. Even with such a high-dimensional parameter space, CosmoPower-JAX provides converged posterior contours in 3 days, as opposed to the estimated 6 years required by standard methods. CosmoPower-JAX is fully written in Python, and we make it publicly available to help the cosmological community meet the accuracy requirements set by next-generation surveys.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/hermasl/Zotero/storage/GXER6GHD/CosmoPower-JAX (Piras+, 2023).pdf;/home/hermasl/Zotero/storage/B3AVPJTS/2305.html}
}

@article{pirasFutureCosmologicalLikelihoodbased2024,
  title = {The Future of Cosmological Likelihood-Based Inference: Accelerated High-Dimensional Parameter Estimation and Model Comparison},
  shorttitle = {The Future of Cosmological Likelihood-Based Inference},
  author = {Piras, Davide and Polanska, Alicja and Mancini, Alessio Spurio and Price, Matthew A. and McEwen, Jason D.},
  year = 2024,
  month = sep,
  journal = {The Open Journal of Astrophysics},
  volume = {7},
  eprint = {2405.12965},
  primaryclass = {astro-ph},
  issn = {2565-6120},
  doi = {10.33232/001c.123368},
  url = {http://arxiv.org/abs/2405.12965},
  urldate = {2025-06-25},
  abstract = {We advocate for a new paradigm of cosmological likelihood-based inference, leveraging recent developments in machine learning and its underlying technology, to accelerate Bayesian inference in high-dimensional settings. Specifically, we combine (i) emulation, where a machine learning model is trained to mimic cosmological observables, e.g. CosmoPower-JAX; (ii) differentiable and probabilistic programming, e.g. JAX and NumPyro, respectively; (iii) scalable Markov chain Monte Carlo (MCMC) sampling techniques that exploit gradients, e.g. Hamiltonian Monte Carlo; and (iv) decoupled and scalable Bayesian model selection techniques that compute the Bayesian evidence purely from posterior samples, e.g. the learned harmonic mean implemented in harmonic. This paradigm allows us to carry out a complete Bayesian analysis, including both parameter estimation and model selection, in a fraction of the time of traditional approaches. First, we demonstrate the application of this paradigm on a simulated cosmic shear analysis for a Stage IV survey in 37- and 39-dimensional parameter spaces, comparing {$\Lambda$}CDM and a dynamical dark energy model (w0waCDM). We recover posterior contours and evidence estimates that are in excellent agreement with those computed by the traditional nested sampling approach while reducing the computational cost from 8 months on 48 CPU cores to 2 days on 12 GPUs. Second, we consider a joint analysis between three simulated next-generation surveys, each performing a 3x2pt analysis, resulting in 157- and 159-dimensional parameter spaces. Standard nested sampling techniques are simply unlikely to be feasible in this high-dimensional setting, requiring a projected 12 years of compute time on 48 CPU cores; on the other hand, the proposed approach only requires 8 days of compute time on 24 GPUs. All packages used in our analyses are publicly available.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/hermasl/Zotero/storage/FCPRPXMX/Piras et al. - 2024 - The future of cosmological likelihood-based infere.pdf}
}

@article{pookkillathBaryonPhysicsTight2019,
  title = {Baryon {{Physics}} and {{Tight Coupling Approximation}} in {{Boltzmann Codes}}},
  author = {Pookkillath, Masroor C. and Felice, Antonio De and Mukohyama, Shinji},
  year = 2019,
  month = dec,
  journal = {Universe},
  volume = {6},
  number = {1},
  eprint = {1906.06831},
  primaryclass = {astro-ph},
  pages = {6},
  issn = {2218-1997},
  doi = {10.3390/universe6010006},
  url = {http://arxiv.org/abs/1906.06831},
  urldate = {2025-02-04},
  abstract = {We provide two derivations of the baryonic equations that can be straightforwardly implemented in existing Einstein--Boltzmann solvers. One of the derivations begins with an action principle, while the other exploits the conservation of the stress-energy tensor. While our result is manifestly covariant and satisfies the Bianchi identities, we point out that this is not the case for the implementation of the seminal work by Ma and Bertschinger and in the existing Boltzmann codes. We also study the tight coupling approximation up to the second order without choosing any gauge using the covariant full baryon equations. We implement the improved baryon equations in a Boltzmann code and investigate the change in the estimate of cosmological parameters by performing an MCMC analysis. With the covariantly correct baryon equations of motion, we find 1\% deviation for the best fit values of the cosmological parameters that should be taken into account. While in this paper, we study the Lambda-CDM model only, our baryon equations can be easily implemented in other models and various modified gravity theories.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {High Energy Physics - Theory},
  file = {/home/hermasl/Zotero/storage/5MAGQKIC/Pookkillath et al. - 2019 - Baryon Physics and Tight Coupling Approximation in.pdf}
}

@article{putterMeasuringSpeedDark2010,
  title = {Measuring the {{Speed}} of {{Dark}}: {{Detecting Dark Energy Perturbations}}},
  shorttitle = {Measuring the {{Speed}} of {{Dark}}},
  author = {de Putter, Roland and Huterer, Dragan and Linder, Eric V.},
  year = 2010,
  month = may,
  journal = {Phys. Rev. D},
  volume = {81},
  number = {10},
  eprint = {1002.1311},
  primaryclass = {astro-ph},
  pages = {103513},
  issn = {1550-7998, 1550-2368},
  doi = {10.1103/PhysRevD.81.103513},
  url = {http://arxiv.org/abs/1002.1311},
  urldate = {2025-07-02},
  abstract = {The nature of dark energy can be probed not only through its equation of state, but also through its microphysics, characterized by the sound speed of perturbations to the dark energy density and pressure. As the sound speed drops below the speed of light, dark energy inhomogeneities increase, affecting both CMB and matter power spectra. We show that current data can put no significant constraints on the value of the sound speed when dark energy is purely a recent phenomenon, but can begin to show more interesting results for early dark energy models. For example, the best fit model for current data has a slight preference for dynamics (w(a)\textbackslash ne-1), degrees of freedom distinct from quintessence (c\_s\textbackslash ne1), and early presence of dark energy (Omega\_ de(a{$<<$}1)\textbackslash ne0). Future data may open a new window on dark energy by measuring its spatial as well as time variation.},
  archiveprefix = {arXiv},
  file = {/home/hermasl/Zotero/storage/6QP4YH8D/Measuring the Speed of Dark (Putter+, 2010).pdf;/home/hermasl/Zotero/storage/9QZ2RSV2/1002.html}
}

@misc{quadgk,
  title = {{{QuadGK}}.Jl: {{Gauss}}--{{Kronrod}} Integration in {{Julia}}},
  author = {Johnson, Steven G.},
  year = 2013,
  url = {https://github.com/JuliaMath/QuadGK.jl}
}

@article{rackauckasDifferentialEquationsjlPerformantFeatureRich2017,
  title = {{{DifferentialEquations}}.Jl -- {{A Performant}} and {{Feature-Rich Ecosystem}} for {{Solving Differential Equations}} in {{Julia}}},
  author = {Rackauckas, Christopher and Nie, Qing},
  year = 2017,
  month = may,
  journal = {Journal of Open Research Software},
  volume = {5},
  pages = {1},
  issn = {2049-9647},
  doi = {10.5334/jors.151},
  url = {https://openresearchsoftware.metajnl.com/articles/10.5334/jors.151},
  urldate = {2025-06-23},
  abstract = {DifferentialEquations.jl is a package for solving differential equations in Julia. It covers discrete equations (function maps, discrete stochastic (Gillespie/Markov) simulations), ordinary differential equations, stochastic differential equations, algebraic differential equations, delay differential equations, hybrid differential equations, jump diffusions, and (stochastic) partial differential equations. Through extensive use of multiple dispatch, metaprogramming, plot recipes, foreign function interfaces (FFI), and call-overloading, DifferentialEquations.jl offers a unified user interface to solve and analyze various forms of differential equations while not sacrificing features or performance. Many modern features are integrated into the solvers, such as allowing arbitrary user-defined number systems for high-precision and arithmetic with physical units, built-in multithreading and parallelism, and symbolic calculation of Jacobians. Integrated into the package is an algorithm testing and benchmarking suite to both ensure accuracy and serve as an easy way for researchers to develop and distribute their own methods. Together, these features build a highly extendable suite which is feature-rich and highly performant.Funding statement: This work was partially supported by NIH grants P50GM76516 and R01GM107264 and NSF grants DMS1562176 and DMS1161621. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-1321846, the National Academies of Science, Engineering, and Medicine via the Ford Foundation, and the National Institutes of Health Award T32 EB009418. Its contents are solely the responsibility of the authors and do not necessarily represent the official views of the NIH.},
  langid = {american},
  file = {/home/hermasl/Zotero/storage/HPIULLGB/DifferentialEquations (Rackauckas+, 2017).pdf}
}

@article{refregierPyCosmoIntegratedCosmological2017a,
  title = {{{PyCosmo}}: {{An Integrated Cosmological Boltzmann Solver}}},
  shorttitle = {{{PyCosmo}}},
  author = {Refregier, Alexandre and Gamper, Lukas and Amara, Adam and Heisenberg, Lavinia},
  year = 2017,
  month = aug,
  journal = {ArXiv},
  eprint = {1708.05177},
  doi = {10.48550/arXiv.1708.05177},
  url = {http://arxiv.org/abs/1708.05177},
  urldate = {2024-11-18},
  abstract = {As wide-field surveys yield ever more precise measurements, cosmology has entered a phase of high precision requiring highly accurate and fast theoretical predictions. At the heart of most cosmological model predictions is a numerical solution of the Einstein-Boltzmann equations governing the evolution of linear perturbations in the Universe. We present PyCosmo, a new Python-based framework to solve this set of equations using a special pur- pose solver based on symbolic manipulations, automatic generation of C++ code and sparsity optimisation. The code uses a consistency relation of the field equations to adapt the time step and does not rely on physical approximations for speed-up. After reviewing the system of first-order linear homogeneous differential equations to be solved, we describe the numerical scheme implemented in PyCosmo. We then compare the predictions and performance of the code for the computation of the transfer functions of cosmological perturbations and compare it to existing cosmological Boltzmann codes. We find that we achieve comparable execution times for comparable accuracies. While PyCosmo does not yet have all the features of other codes, our approach is complementary to existing cosmological Boltzmann solvers and can be used as an independent test of their numerical solutions. The symbolic representation of the Einstein-Boltzmann equation system in PyCosmo provides a convenient interface for implementing extended cosmological models. We also discuss how the PyCosmo framework can also be used as a general framework to compute cosmological quantities as well as observables for both interactive and high-performance batch jobs applications. Information about the PyCosmo package and future code releases are available at http://www.cosmology.ethz.ch/research/software-lab.html.},
  archiveprefix = {arXiv},
  file = {/home/hermasl/Zotero/storage/H29Y4PKD/PyCosmo (Refregier+, 2017).pdf;/home/hermasl/Zotero/storage/V48BNBQ7/1708.html}
}

@article{revelsForwardModeAutomaticDifferentiation2016,
  title = {Forward-{{Mode Automatic Differentiation}} in {{Julia}}},
  author = {Revels, Jarrett and Lubin, Miles and Papamarkou, Theodore},
  year = 2016,
  month = jul,
  journal = {ArXiv},
  eprint = {1607.07892},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1607.07892},
  url = {http://arxiv.org/abs/1607.07892},
  urldate = {2025-06-23},
  abstract = {We present ForwardDiff, a Julia package for forward-mode automatic differentiation (AD) featuring performance competitive with low-level languages like C++. Unlike recently developed AD tools in other popular high-level languages such as Python and MATLAB, ForwardDiff takes advantage of just-in-time (JIT) compilation to transparently recompile AD-unaware user code, enabling efficient support for higher-order differentiation and differentiation using custom number types (including complex numbers). For gradient and Jacobian calculations, ForwardDiff provides a variant of vector-forward mode that avoids expensive heap allocation and makes better use of memory bandwidth than traditional vector mode. In our numerical experiments, we demonstrate that for nontrivially large dimensions, ForwardDiff's gradient computations can be faster than a reverse-mode implementation from the Python-based autograd package. We also illustrate how ForwardDiff is used effectively within JuMP, a modeling language for optimization. According to our usage statistics, 41 unique repositories on GitHub depend on ForwardDiff, with users from diverse fields such as astronomy, optimization, finite element analysis, and statistics. This document is an extended abstract that has been accepted for presentation at the AD2016 7th International Conference on Algorithmic Differentiation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Mathematical Software},
  file = {/home/hermasl/Zotero/storage/PMP6TR9F/Forward-Mode Automatic Differentiation in Julia (Revels+, 2016).pdf;/home/hermasl/Zotero/storage/7YVHTW9C/1607.html}
}

@misc{schmittSympy2cSymbolicExpressions2022,
  title = {Sympy2c: From Symbolic Expressions to Fast {{C}}/{{C}}++ Functions and {{ODE}} Solvers in {{Python}}},
  shorttitle = {Sympy2c},
  author = {Schmitt, Uwe and Moser, Beatrice and Lorenz, Christiane S. and Refregier, Alexandre},
  year = 2022,
  month = mar,
  number = {arXiv:2203.11945},
  eprint = {2203.11945},
  primaryclass = {astro-ph, physics:physics},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2203.11945},
  urldate = {2024-03-01},
  abstract = {Computer algebra systems play an important role in science as they facilitate the development of new theoretical models. The resulting symbolic equations are often implemented in a compiled programming language in order to provide fast and portable codes for practical applications. We describe sympy2c, a new Python package designed to bridge the gap between the symbolic development and the numerical implementation of a theoretical model. sympy2c translates symbolic equations implemented in the SymPy Python package to C/C++ code that is optimized using symbolic transformations. The resulting functions can be conveniently used as an extension module in Python. sympy2c is used within the PyCosmo Python package to solve the Einstein-Boltzmann equations, a large system of ODEs describing the evolution of linear perturbations in the Universe. After reviewing the functionalities and usage of sympy2c, we describe its implementation and optimization strategies. This includes, in particular, a novel approach to generate optimized ODE solvers making use of the sparsity of the symbolic Jacobian matrix. We demonstrate its performance using the Einstein-Boltzmann equations as a test case. sympy2c is widely applicable and may prove useful for various areas of computational physics. sympy2c is publicly available at https://cosmology.ethz.ch/research/software-lab/sympy2c.html},
  archiveprefix = {arXiv},
  keywords = {_tablet,Computer Science - Mathematical Software,Physics - Computational Physics},
  file = {/home/hermasl/Zotero/storage/AJBWGQH4/sympy2c (Schmitt+, 2022).pdf;/home/hermasl/Zotero/storage/TKYGI2S2/2203.html}
}

@article{schonebergTraditionalLineofSightApproach2018,
  title = {Beyond the Traditional {{Line-of-Sight}} Approach of Cosmological Angular Statistics},
  author = {Sch{\"o}neberg, Nils and Simonovi{\'c}, Marko and Lesgourgues, Julien and Zaldarriaga, Matias},
  year = 2018,
  month = oct,
  journal = {JCAP},
  volume = {2018},
  number = {10},
  eprint = {1807.09540},
  primaryclass = {astro-ph},
  pages = {047--047},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2018/10/047},
  url = {http://arxiv.org/abs/1807.09540},
  urldate = {2024-12-04},
  abstract = {We present a new efficient method to compute the angular power spectra of large-scale structure observables that circumvents the numerical integration over Bessel functions, expanding on a recently proposed algorithm based on FFTlog. This new approach has better convergence properties. The method is explicitly implemented in the CLASS code for the case of number count C 's (including redshift-space distortions, weak lensing, and all other relativistic corrections) and cosmic shear C 's. In both cases our approach speeds up the calculation of the exact C 's (without the Limber approximation) by a factor of order 400 at a fixed precision target of 0.1\%.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {_tablet},
  file = {/home/hermasl/Zotero/storage/NKW22Z8U/Beyond the traditional Line-of-Sight approach of cosmological angular statistics (Schöneberg+, 2018).pdf}
}

@article{scolnicCompleteLightcurveSample2018,
  title = {The {{Complete Light-curve Sample}} of {{Spectroscopically Confirmed Type Ia Supernovae}} from {{Pan-STARRS1}} and {{Cosmological Constraints}} from {{The Combined Pantheon Sample}}},
  author = {Scolnic, D. M. and Jones, D. O. and Rest, A. and Pan, Y. C. and Chornock, R. and Foley, R. J. and Huber, M. E. and Kessler, R. and Narayan, G. and Riess, A. G. and Rodney, S. and Berger, E. and Brout, D. J. and Challis, P. J. and Drout, M. and Finkbeiner, D. and Lunnan, R. and Kirshner, R. P. and Sanders, N. E. and Schlafly, E. and Smartt, S. and Stubbs, C. W. and Tonry, J. and {Wood-Vasey}, W. M. and Foley, M. and Hand, J. and Johnson, E. and Burgett, W. S. and Chambers, K. C. and Draper, P. W. and Hodapp, K. W. and Kaiser, N. and Kudritzki, R. P. and Magnier, E. A. and Metcalfe, N. and Bresolin, F. and Gall, E. and Kotak, R. and McCrum, M. and Smith, K. W.},
  year = 2018,
  month = jun,
  journal = {ApJ},
  volume = {859},
  number = {2},
  eprint = {1710.00845},
  primaryclass = {astro-ph},
  pages = {101},
  issn = {0004-637X, 1538-4357},
  doi = {10.3847/1538-4357/aab9bb},
  url = {http://arxiv.org/abs/1710.00845},
  urldate = {2025-06-23},
  abstract = {We present optical light curves, redshifts, and classifications for 365 spectroscopically confirmed Type Ia supernovae (SNe Ia) discovered by the Pan-STARRS1 (PS1) Medium Deep Survey. We detail improvements to the PS1 SN photometry, astrometry and calibration that reduce the systematic uncertainties in the PS1 SN Ia distances. We combine the subset of 279 PS1 SN Ia (\$0.03 {$<$} z {$<$} 0.68\$) with useful distance estimates of SN Ia from SDSS, SNLS, various low-z and HST samples to form the largest combined sample of SN Ia consisting of a total of 1048 SN Ia ranging from \$0.01 {$<$} z {$<$} 2.3\$, which we call the `Pantheon Sample'. When combining Planck 2015 CMB measurements with the Pantheon SN sample, we find \$\textbackslash Omega\_m=0.307\textbackslash pm0.012\$ and \$w = -1.026\textbackslash pm0.041\$ for the wCDM model. When the SN and CMB constraints are combined with constraints from BAO and local H0 measurements, the analysis yields the most precise measurement of dark energy to date: \$w0 = -1.007\textbackslash pm 0.089\$ and \$wa = -0.222 \textbackslash pm0.407\$ for the w0waCDM model. Tension with a cosmological constant previously seen in an analysis of PS1 and low-z SNe has diminished after an increase of \$2\textbackslash times\$ in the statistics of the PS1 sample, improved calibration and photometry, and stricter light-curve quality cuts. We find the systematic uncertainties in our measurements of dark energy are almost as large as the statistical uncertainties, primarily due to limitations of modeling the low-redshift sample. This must be addressed for future progress in using SN Ia to measure dark energy.},
  archiveprefix = {arXiv},
  file = {/home/hermasl/Zotero/storage/5FDIR3GE/The Complete Light-curve Sample of Spectroscopically Confirmed Type Ia (Scolnic+, 2018).pdf;/home/hermasl/Zotero/storage/R3SDTCTQ/1710.html}
}

@article{scottMatterTemperatureCosmological2009,
  title = {Matter Temperature after Cosmological Recombination},
  author = {Scott, Douglas and Moss, Adam},
  year = 2009,
  month = jul,
  journal = {MNRAS},
  volume = {397},
  number = {1},
  eprint = {0902.3438},
  primaryclass = {astro-ph},
  pages = {445--446},
  issn = {00358711, 13652966},
  doi = {10.1111/j.1365-2966.2009.14939.x},
  url = {http://arxiv.org/abs/0902.3438},
  urldate = {2025-09-16},
  abstract = {The temperature of the atomic matter in the Universe is held to that of the Cosmic Background radiation until decoupling at z\textasciitilde 100. After this it cools faster than the radiation (\textbackslash propto(1+z)\textasciicircum 2 rather than (1+z)) and would have fallen to about 20mK today if astrophysical feedback processes had not heated up the interglactic medium. We show how the derivative of the Compton coupling equation helps numerically to follow the decoupling process.},
  archiveprefix = {arXiv},
  file = {/home/hermasl/Zotero/storage/ZU2SS8B3/Scott and Moss - 2009 - Matter temperature after cosmological recombination.pdf;/home/hermasl/Zotero/storage/SD9NA2CS/0902.html}
}

@article{seagerHowExactlyDid2000,
  title = {How Exactly Did the {{Universe}} Become Neutral?},
  author = {Seager, Sara and Sasselov, Dimitar D. and Scott, Douglas},
  year = 2000,
  month = jun,
  journal = {ApJS},
  volume = {128},
  number = {2},
  eprint = {astro-ph/9912182},
  pages = {407--430},
  issn = {0067-0049, 1538-4365},
  doi = {10.1086/313388},
  url = {http://arxiv.org/abs/astro-ph/9912182},
  urldate = {2024-06-14},
  abstract = {We present a refined treatment of H, He I, and He II recombination in the early Universe. The difference from previous calculations is that we use multi-level atoms and evolve the population of each level with redshift by including all bound-bound and bound-free transitions. In this framework we follow several hundred atomic energy levels for H, He I, and He II combined. The main improvements of this method over previous recombination calculations are: (1) allowing excited atomic level populations to depart from an equilibrium distribution; (2) replacing the total recombination coefficient with recombination to and photoionization from each level directly at each redshift step; and (3) correct treatment of the He I atom, including the triplet and singlet states. We find that the ionization fraction x\_e = n\_e/n\_H is approximately 10\% smaller at redshifts {$<$}\textasciitilde 800 than in previous calculations, due to the non-equilibrium of the excited states of H, which is caused by the strong but cool radiation field at those redshifts. In addition we find that He I recombination is delayed compared with previous calculations, and occurs only just before H recombination. These changes in turn can affect the predicted power spectrum of microwave anisotropies at the few percent level. Other improvements such as including molecular and ionic species of H, including complete heating and cooling terms for the evolution of the matter temperature, including collisional rates, and including feedback of the secondary spectral distortions on the radiation field, produce negligible change to x\_e. The lower x\_e at low z found in this work affects the abundances of H molecular and ionic species by 10-25\%. However this difference is probably not larger than other uncertainties in the reaction rates.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics},
  file = {/home/hermasl/Zotero/storage/WNFLFHKG/How exactly did the Universe become neutral (Seager+, 2000).pdf;/home/hermasl/Zotero/storage/MJB9JAGI/9912182.html}
}

@article{seagerNewCalculationRecombination1999,
  title = {A {{New Calculation}} of the {{Recombination Epoch}}},
  author = {Seager, Sara and Sasselov, Dimitar D. and Scott, Douglas},
  year = 1999,
  month = sep,
  journal = {ApJ},
  volume = {523},
  number = {1},
  eprint = {astro-ph/9909275},
  pages = {L1-L5},
  issn = {0004637X},
  doi = {10.1086/312250},
  url = {http://arxiv.org/abs/astro-ph/9909275},
  urldate = {2024-06-14},
  abstract = {We have developed an improved recombination calculation of H, He I, and He II in the early Universe which involves a line-by-line treatment of each atomic level. We find two major differences compared with previous calculations. Firstly, the ionization fraction xe is approximately 10\% smaller for redshifts {$\sim<$} 800, due to non-equilibrium processes in the excited states of H. Secondly, He I recombination is much slower than previously thought, and is delayed until just before H recombines. We describe the basic physics behind the new results and present a simple way to reproduce our calculation. This should enable fast computation of the ionization history (and quantities such as the power spectrum of CMB anisotropies which depend on it) for arbitrary cosmologies, without the need to consider the hundreds of atomic levels used in our complete model.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Astrophysics},
  file = {/home/hermasl/Zotero/storage/K9B8BKHG/Seager et al. - 1999 - A New Calculation of the Recombination Epoch.pdf}
}

@article{seljakLineSightApproach1996,
  title = {A {{Line}} of {{Sight Approach}} to {{Cosmic Microwave Background Anisotropies}}},
  author = {Seljak, Uros and Zaldarriaga, Matias},
  year = 1996,
  month = oct,
  journal = {ApJ},
  volume = {469},
  eprint = {astro-ph/9603033},
  pages = {437},
  issn = {0004-637X, 1538-4357},
  doi = {10.1086/177793},
  url = {http://arxiv.org/abs/astro-ph/9603033},
  urldate = {2024-11-13},
  abstract = {We present a new method for calculating linear cosmic microwave background (CMB) anisotropy spectra based on integration over sources along the photon past light cone. In this approach the temperature anisotropy is written as a time integral over the product of a geometrical term and a source term. The geometrical term is given by radial eigenfunctions which do not depend on the particular cosmological model. The source term can be expressed in terms of photon, baryon and metric perturbations, all of which can be calculated using a small number of differential equations. This split clearly separates between the dynamical and geometrical effects on the CMB anisotropies. More importantly, it allows to significantly reduce the computational time compared to standard methods. This is achieved because the source term, which depends on the model and is generally the most time consuming part of calculation, is a slowly varying function of wavelength and needs to be evaluated only in a small number of points. The geometrical term, which oscillates much more rapidly than the source term, does not depend on the particular model and can be precomputed in advance. Standard methods that do not separate the two terms and require a much higher number of evaluations. The new method leads to about two orders of magnitude reduction in CPU time when compared to standard methods and typically requires a few minutes on a workstation for a single model. The method should be especially useful for accurate determinations of cosmological parameters from CMB anisotropy and polarization measurements that will become possible with the next generation of experiments. A programm implementing this method can be obtained from the authors.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Astrophysics},
  file = {/home/hermasl/Zotero/storage/H2DWW59N/Seljak and Zaldarriaga - 1996 - A Line of Sight Approach to Cosmic Microwave Backg.pdf}
}

@article{seljakOptimalExtractionCosmological2017,
  title = {Towards Optimal Extraction of Cosmological Information from Nonlinear Data},
  author = {Seljak, Uros and Aslanyan, Grigor and Feng, Yu and Modi, Chirag},
  year = 2017,
  month = dec,
  journal = {JCAP},
  volume = {12},
  eprint = {1706.06645},
  primaryclass = {astro-ph},
  pages = {009--009},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2017/12/009},
  url = {http://arxiv.org/abs/1706.06645},
  urldate = {2025-06-25},
  abstract = {One of the main unsolved problems of cosmology is how to maximize the extraction of information from nonlinear data. If the data are nonlinear the usual approach is to employ a sequence of statistics (N-point statistics, counting statistics of clusters, density peaks or voids etc.), along with the corresponding covariance matrices. However, this approach is computationally prohibitive and has not been shown to be exhaustive in terms of information content. Here we instead develop a hierarchical Bayesian approach, expanding the likelihood around the maximum posterior of linear modes, which we solve for using optimization methods. By integrating out the modes using perturbative expansion of the likelihood we construct an initial power spectrum estimator, which for a fixed forward model contains all the cosmological information if the initial modes are gaussian distributed. We develop a method to construct the window and covariance matrix such that the estimator is explicitly unbiased and nearly optimal. We then generalize the method to include the forward model parameters, including cosmological and nuisance parameters, and primordial non-gaussianity. We apply the method in the simplified context of nonlinear structure formation, using either simplified 2-LPT dynamics or N-body simulations as the nonlinear mapping between linear and nonlinear density, and 2-LPT dynamics in the optimization steps used to reconstruct the initial density modes. We demonstrate that the method gives an unbiased estimator of the initial power spectrum, providing among other a near optimal reconstruction of linear baryonic acoustic oscillations.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/VS5RAXKD/Seljak et al. - 2017 - Towards optimal extraction of cosmological informa.pdf}
}

@article{shampineMATLABODESuite1997,
  title = {The {{MATLAB ODE Suite}}},
  author = {Shampine, Lawrence F. and Reichelt, Mark W.},
  year = 1997,
  month = jan,
  journal = {SIAM J. Sci. Comput.},
  volume = {18},
  number = {1},
  pages = {1--22},
  issn = {1064-8275, 1095-7197},
  doi = {10.1137/S1064827594276424},
  url = {http://epubs.siam.org/doi/10.1137/S1064827594276424},
  urldate = {2025-12-09},
  abstract = {This paper describes mathematical and software developments for a suite of programs for solving ordinary di erential equations in Matlab.},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/GZ4FC4F4/Shampine and Reichelt - 1997 - The MATLAB ODE Suite.pdf}
}

@article{sletmoenSymBoltzjlSymbolicnumericApproximationfree2025,
  title = {{{SymBoltz}}.Jl: A Symbolic-Numeric, Approximation-Free and Differentiable Linear {{Einstein-Boltzmann}} Solver},
  shorttitle = {{{SymBoltz}}.Jl},
  author = {Sletmoen, Herman},
  year = 2025,
  month = sep,
  journal = {ArXiv},
  eprint = {2509.24740},
  primaryclass = {astro-ph},
  doi = {10.48550/arXiv.2509.24740},
  url = {http://arxiv.org/abs/2509.24740},
  urldate = {2025-09-30},
  abstract = {SymBoltz is a new Julia package that solves the linear Einstein-Boltzmann equations. It features a symbolic-numeric interface for specifying equations, is free of approximation switching schemes and is compatible with automatic differentiation. Cosmological models are built from replaceable physical components in a way that scales well in model space. The modeler should simply write down their equations, and SymBoltz solves them and eliminates much of the friction in the process. SymBoltz enables up to 100x shorter model definitions compared to browsing equivalent files in CLASS. Symbolic knowledge enables powerful automation of tasks, such as separating computational stages like the background and perturbations, generating the Jacobian matrix and its sparsity pattern, and interpolating arbitrary expressions from the solution. Modern implicit solvers integrate the full stiff equations at all times, reducing slowdowns by taking long time steps, reusing the Jacobian and LU-factorizing it over several time steps, and using fast linear system solvers. Automatic differentiation gives exact derivatives of any output with respect to any input, which is important for gradient-based Markov chain Monte Carlo methods in large parameter spaces, training of emulators, Fisher forecasting and sensitivity analysis. These features are useful in their own rights, but also reinforce each other in a synergy. Results agree with established codes like CLASS and CAMB. With more work, SymBoltz can grow into an integrated symbolic-numeric cosmological modeling environment with a large library of models that delivers differentiable output as fast as other codes. SymBoltz is available at https://github.com/hersle/SymBoltz.jl with single-command installation and extensive documentation, and welcomes questions, suggestions and contributions.},
  archiveprefix = {arXiv},
  keywords = {Physics - Computational Physics},
  file = {/home/hermasl/Zotero/storage/YBRM4SKH/Sletmoen - 2025 - SymBoltz.jl a symbolic-numeric, approximation-free and differentiable linear Einstein-Boltzmann sol.pdf;/home/hermasl/Zotero/storage/QRZNCRFT/2509.html}
}

@article{smithStableClusteringHalo2003,
  title = {Stable Clustering, the Halo Model and Nonlinear Cosmological Power Spectra},
  author = {Smith, R. E. and Peacock, J. A. and Jenkins, A. and White, S. D. M. and Frenk, C. S. and Pearce, F. R. and Thomas, P. A. and Efstathiou, G. and Couchmann, H. M. P. and Consortium, The Virgo},
  year = 2003,
  month = jun,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {341},
  number = {4},
  eprint = {astro-ph/0207664},
  pages = {1311--1332},
  issn = {0035-8711, 1365-2966},
  doi = {10.1046/j.1365-8711.2003.06503.x},
  url = {http://arxiv.org/abs/astro-ph/0207664},
  urldate = {2025-11-11},
  abstract = {We present the results of a large library of cosmological N-body simulations, using power-law initial spectra. The nonlinear evolution of the matter power spectra is compared with the predictions of existing analytic scaling formulae based on the work of Hamilton et al. The scaling approach has assumed that highly nonlinear structures obey `stable clustering' and are frozen in proper coordinates. Our results show that, when transformed under the self-similarity scaling, the scale-free spectra define a nonlinear locus that is clearly shallower than would be required under stable clustering. Furthermore, the small-scale nonlinear power increases as both the power-spectrum index n and the density parameter Omega decrease, and this evolution is not well accounted for by the previous scaling formulae. This breakdown of stable clustering can be understood as resulting from the modification of dark-matter haloes by continuing mergers. These effects are naturally included in the analytic `halo model' for nonlinear structure; using this approach we are able to fit both our scale-free results and also our previous CDM data. This approach is more accurate than the commonly-used Peacock--Dodds formula and should be applicable to more general power spectra. Code to evaluate nonlinear power spectra using this method is available from http://as1.chem.nottingham.ac.uk/\textasciitilde res/software.html Following publication, we will make the power-law simulation data available through the Virgo website http://www.mpa-garching.mpg.de/Virgo},
  archiveprefix = {arXiv},
  keywords = {Astrophysics},
  file = {/home/hermasl/Zotero/storage/EJTWIHBP/Smith et al. - 2003 - Stable clustering, the halo model and nonlinear cosmological power spectra.pdf;/home/hermasl/Zotero/storage/GGGLFDZJ/0207664.html}
}

@misc{spergelWideFieldInfrarRedSurvey2015,
  title = {Wide-{{Field InfrarRed Survey Telescope-Astrophysics Focused Telescope Assets WFIRST-AFTA}} 2015 {{Report}}},
  author = {Spergel, D. and Gehrels, N. and Baltay, C. and Bennett, D. and Breckinridge, J. and Donahue, M. and Dressler, A. and Gaudi, B. S. and Greene, T. and Guyon, O. and Hirata, C. and Kalirai, J. and Kasdin, N. J. and Macintosh, B. and Moos, W. and Perlmutter, S. and Postman, M. and Rauscher, B. and Rhodes, J. and Wang, Y. and Weinberg, D. and Benford, D. and Hudson, M. and Jeong, W.-S. and Mellier, Y. and Traub, W. and Yamada, T. and Capak, P. and Colbert, J. and Masters, D. and Penny, M. and Savransky, D. and Stern, D. and Zimmerman, N. and Barry, R. and Bartusek, L. and Carpenter, K. and Cheng, E. and Content, D. and Dekens, F. and Demers, R. and Grady, K. and Jackson, C. and Kuan, G. and Kruk, J. and Melton, M. and Nemati, B. and Parvin, B. and Poberezhskiy, I. and Peddie, C. and Ruffa, J. and Wallace, J. K. and Whipple, A. and Wollack, E. and Zhao, F.},
  year = 2015,
  month = mar,
  number = {arXiv:1503.03757},
  eprint = {1503.03757},
  primaryclass = {astro-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1503.03757},
  url = {http://arxiv.org/abs/1503.03757},
  urldate = {2025-06-25},
  abstract = {This report describes the 2014 study by the Science Definition Team (SDT) of the Wide-Field Infrared Survey Telescope (WFIRST) mission. It is a space observatory that will address the most compelling scientific problems in dark energy, exoplanets and general astrophysics using a 2.4-m telescope with a wide-field infrared instrument and an optical coronagraph. The Astro2010 Decadal Survey recommended a Wide Field Infrared Survey Telescope as its top priority for a new large space mission. As conceived by the decadal survey, WFIRST would carry out a dark energy science program, a microlensing program to determine the demographics of exoplanets, and a general observing program utilizing its ultra wide field. In October 2012, NASA chartered a Science Definition Team (SDT) to produce, in collaboration with the WFIRST Study Office at GSFC and the Program Office at JPL, a Design Reference Mission (DRM) for an implementation of WFIRST using one of the 2.4-m, Hubble-quality telescope assemblies recently made available to NASA. This DRM builds on the work of the earlier WFIRST SDT, reported by Green et al. (2012) and the previous WFIRST-2.4 DRM, reported by Spergel et. (2013). The 2.4-m primary mirror enables a mission with greater sensitivity and higher angular resolution than the 1.3-m and 1.1-m designs considered previously, increasing both the science return of the primary surveys and the capabilities of WFIRST as a Guest Observer facility. The addition of an on-axis coronagraphic instrument to the baseline design enables imaging and spectroscopic studies of planets around nearby stars.},
  archiveprefix = {arXiv},
  file = {/home/hermasl/Zotero/storage/VHJ8TQDW/Wide-Field InfrarRed Survey Telescope-Astrophysics Focused Telescope Assets (Spergel+, 2015).pdf;/home/hermasl/Zotero/storage/Z6K3TCLA/1503.html}
}

@article{steinebachConstructionRosenbrockWanner2023,
  title = {Construction of {{Rosenbrock}}--{{Wanner}} Method {{Rodas5P}} and Numerical Benchmarks within the {{Julia Differential Equations}} Package},
  author = {Steinebach, Gerd},
  year = 2023,
  month = apr,
  journal = {Bit Numer Math},
  volume = {63},
  number = {2},
  pages = {27},
  issn = {1572-9125},
  doi = {10.1007/s10543-023-00967-x},
  url = {https://doi.org/10.1007/s10543-023-00967-x},
  urldate = {2025-12-16},
  abstract = {Rosenbrock--Wanner methods for systems of stiff ordinary differential equations are well known since the seventies. They have been continuously developed and are efficient for differential-algebraic equations of index-1, as well. Their disadvantage that the Jacobian matrix has to be updated in every time step becomes more and more obsolete when automatic differentiation is used. Especially the family of Rodas methods has proven to be a standard in the Julia package DifferentialEquations. However, the fifth-order Rodas5 method undergoes order reduction for certain problem classes. Therefore, the goal of this paper is to compute a new set of coefficients for Rodas5 such that this order reduction is reduced. The procedure is similar to the derivation of the methods Rodas4P and Rodas4P2. In addition, it is possible to provide new dense output formulas for Rodas5 and the new method Rodas5P. Numerical tests show that for higher accuracy requirements Rodas5P always belongs to the best methods within the Rodas family.},
  langid = {english},
  keywords = {65L04,65L80,68N15,Index-1 DAEs,Julia package Differential Equations,Order reduction,Rodas5,Rosenbrock-Wanner methods},
  file = {/home/hermasl/Zotero/storage/X7YUFQQH/Steinebach - 2023 - Construction of Rosenbrock–Wanner method Rodas5P and numerical benchmarks within the Julia Different.pdf}
}

@article{steinebachOrderreductionROWmethodsDAEs1995,
  title = {Order-Reduction of {{ROW-methods}} for {{DAEs}} and Method of Lines Applications},
  author = {Steinebach, G.},
  year = 1995,
  journal = {Technische Hochschule Darmstadt},
  number = {1741},
  url = {https://pub.h-brs.de/frontdoor/index/index/docId/1548},
  urldate = {2025-07-04},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/LNT5F4BP/1548.html}
}

@article{takahashiRevisingHalofitModel2012,
  title = {Revising the {{Halofit Model}} for the {{Nonlinear Matter Power Spectrum}}},
  author = {Takahashi, Ryuichi and Sato, Masanori and Nishimichi, Takahiro and Taruya, Atsushi and Oguri, Masamune},
  year = 2012,
  month = dec,
  journal = {ApJ},
  volume = {761},
  number = {2},
  eprint = {1208.2701},
  primaryclass = {astro-ph},
  pages = {152},
  issn = {0004-637X, 1538-4357},
  doi = {10.1088/0004-637X/761/2/152},
  url = {http://arxiv.org/abs/1208.2701},
  urldate = {2025-11-11},
  abstract = {Based on a suite of state-of-the-art high-resolution \$N\$-body simulations, we revisit the so-called halofit model (Smith et al. 2003) as an accurate fitting formula for the nonlinear matter power spectrum. While the halofit model has been frequently used as a standard cosmological tool to predict the nonlinear matter power spectrum in a universe dominated by cold dark matter, its precision has been limited by the low-resolution of \$N\$-body simulations used to determine the fitting parameters, suggesting the necessity of improved fitting formula at small scales for future cosmological studies. We run high-resolution \$N\$-body simulations for 16 cosmological models around the Wilkinson Microwave Anisotropy Probe (WMAP) best-fit cosmological parameters (1, 3, 5, and 7 year results), including dark energy models with a constant equation of state. The simulation results are used to re-calibrate the fitting parameters of the halofit model so as to reproduce small-scale power spectra of the \$N\$-body simulations, while keeping the precision at large scales. The revised fitting formula provides an accurate prediction of the nonlinear matter power spectrum in a wide range of wavenumber (\$k \textbackslash leq 30h\$\textbackslash,Mpc\$\textasciicircum\textbraceleft -1\textbraceright\$) at redshifts \$0 \textbackslash leq z \textbackslash leq 10\$, with 5\% precision for \$k\textbackslash leq1 h\$ Mpc\$\textasciicircum\textbraceleft -1\textbraceright\$ at \$0 \textbackslash leq z \textbackslash leq 10\$ and 10\% for \$1 \textbackslash leq k\textbackslash leq 10 h\$ Mpc\$\textasciicircum\textbraceleft -1\textbraceright{} \$ at \$0 \textbackslash leq z \textbackslash leq 3\$. We discuss the impact of the improved halofit model on weak lensing power spectra and correlation functions, and show that the improved model better reproduces ray-tracing simulation results.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics,General Relativity and Quantum Cosmology},
  file = {/home/hermasl/Zotero/storage/2F9BF892/Takahashi et al. - 2012 - Revising the Halofit Model for the Nonlinear Matter Power Spectrum.pdf;/home/hermasl/Zotero/storage/K8BZHLVD/1208.html}
}

@misc{tarsitanoPredictingCosmologicalObservables2020a,
  title = {Predicting {{Cosmological Observables}} with {{PyCosmo}}},
  author = {Tarsitano, F. and Schmitt, U. and Refregier, A. and Fluri, J. and Sgier, R. and Nicola, A. and Herbel, J. and Amara, A. and Kacprzak, T. and Heisenberg, L.},
  year = 2020,
  month = may,
  number = {arXiv:2005.00543},
  eprint = {2005.00543},
  primaryclass = {astro-ph},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2005.00543},
  urldate = {2024-11-18},
  abstract = {Current and upcoming cosmological experiments open a new era of precision cosmology, thus demanding accurate theoretical predictions for cosmological observables. Because of the complexity of the codes delivering such predictions, reaching a high level of numerical accuracy is challenging. Among the codes already fulfilling this task, PyCosmo is a Python-based framework providing solutions to the Einstein-Boltzmann equations and accurate predictions for cosmological observables. In this work, we first describe how the observables are implemented. Then, we check the accuracy of the theoretical predictions for background quantities, power spectra and Limber and beyond-Limber angular power spectra by comparison with other codes: the Core Cosmology Library (CCL), CLASS, HMCode and iCosmo. In our analysis we quantify the agreement of PyCosmo with the other codes, for a range of cosmological models, monitored through a series of unit tests. PyCosmo, conceived as a multi-purpose cosmology calculation tool in Python, is designed to be interactive and user-friendly. A current version of the code (without the Boltzmann Solver) is publicly available and can be used interactively on the platform PyCosmo Hub, all accessible from this link: (https://cosmology.ethz.ch/research/software-lab/PyCosmo. html). On the hub the users can perform their own computations using Jupyter Notebooks without the need of installing any software, access to the results presented in this work and benefit from tutorial notebooks illustrating the usage of the code. The link above also redirects to the code release and documentation.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/BJXY8JY8/Tarsitano et al. - 2020 - Predicting Cosmological Observables with PyCosmo.pdf}
}

@article{thesimonsobservatorycollaborationSimonsObservatoryScience2019,
  title = {The {{Simons Observatory}}: {{Science}} Goals and Forecasts},
  shorttitle = {The {{Simons Observatory}}},
  author = {{The Simons Observatory Collaboration}},
  year = 2019,
  month = feb,
  journal = {JCAP},
  volume = {02},
  eprint = {1808.07445},
  primaryclass = {astro-ph},
  pages = {056--056},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2019/02/056},
  urldate = {2025-06-25},
  abstract = {The Simons Observatory (SO) is a new cosmic microwave background experiment being built on Cerro Toco in Chile, due to begin observations in the early 2020s. We describe the scientific goals of the experiment, motivate the design, and forecast its performance. SO will measure the temperature and polarization anisotropy of the cosmic microwave background in six frequency bands: 27, 39, 93, 145, 225 and 280 GHz. The initial configuration of SO will have three small-aperture 0.5-m telescopes (SATs) and one large-aperture 6-m telescope (LAT), with a total of 60,000 cryogenic bolometers. Our key science goals are to characterize the primordial perturbations, measure the number of relativistic species and the mass of neutrinos, test for deviations from a cosmological constant, improve our understanding of galaxy evolution, and constrain the duration of reionization. The SATs will target the largest angular scales observable from Chile, mapping \textasciitilde 10\% of the sky to a white noise level of 2 \$\textbackslash mu\$K-arcmin in combined 93 and 145 GHz bands, to measure the primordial tensor-to-scalar ratio, \$r\$, at a target level of \$\textbackslash sigma(r)=0.003\$. The LAT will map \textasciitilde 40\% of the sky at arcminute angular resolution to an expected white noise level of 6 \$\textbackslash mu\$K-arcmin in combined 93 and 145 GHz bands, overlapping with the majority of the LSST sky region and partially with DESI. With up to an order of magnitude lower polarization noise than maps from the Planck satellite, the high-resolution sky maps will constrain cosmological parameters derived from the damping tail, gravitational lensing of the microwave background, the primordial bispectrum, and the thermal and kinematic Sunyaev-Zel'dovich effects, and will aid in delensing the large-angle polarization signal to measure the tensor-to-scalar ratio. The survey will also provide a legacy catalog of 16,000 galaxy clusters and more than 20,000 extragalactic sources.},
  archiveprefix = {arXiv}
}

@article{tramOptimalPolarisationEquations2013,
  title = {Optimal Polarisation Equations in {{FLRW}} Universes},
  author = {Tram, Thomas and Lesgourgues, Julien},
  year = 2013,
  month = oct,
  journal = {J. Cosmol. Astropart. Phys.},
  volume = {2013},
  number = {10},
  eprint = {1305.3261},
  primaryclass = {astro-ph},
  pages = {002--002},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2013/10/002},
  url = {http://arxiv.org/abs/1305.3261},
  urldate = {2025-10-27},
  abstract = {This paper presents the linearised Boltzmann equation for photons for scalar, vector and tensor perturbations in flat, open and closed FLRW cosmologies. We show that E- and B-mode polarisation for all types can be computed using only a single hierarchy. This was previously shown explicitly for tensor modes in flat cosmologies but not for vectors, and not for non-flat cosmologies.},
  archiveprefix = {arXiv},
  file = {/home/hermasl/Zotero/storage/4VUYYDPR/Tram and Lesgourgues - 2013 - Optimal polarisation equations in FLRW universes.pdf;/home/hermasl/Zotero/storage/ZZQ2W8WH/1305.html}
}

@misc{utkarshParallelizingExplicitImplicit2022,
  title = {Parallelizing {{Explicit}} and {{Implicit Extrapolation Methods}} for {{Ordinary Differential Equations}}},
  author = {Utkarsh and Elrod, Chris and Ma, Yingbo and Rackauckas, Christopher},
  year = 2022,
  month = sep,
  number = {arXiv:2207.08135},
  eprint = {2207.08135},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.08135},
  url = {http://arxiv.org/abs/2207.08135},
  urldate = {2025-07-04},
  abstract = {Numerically solving ordinary differential equations (ODEs) is a naturally serial process and as a result the vast majority of ODE solver software are serial. In this manuscript we developed a set of parallelized ODE solvers using extrapolation methods which exploit ``parallelism within the method'' so that arbitrary user ODEs can be parallelized. We describe the specific choices made in the implementation of the explicit and implicit extrapolation methods which allow for generating low overhead static schedules to then exploit with optimized multithreaded implementations. We demonstrate that while the multithreading gives a noticeable acceleration on both explicit and implicit problems, the explicit parallel extrapolation methods gave no significant improvement over state-of-the-art even with a multi-threading advantage against current optimized high order Runge-Kutta tableaus. However, we demonstrate that the implicit parallel extrapolation methods are able to achieve state-of-theart performance (2x-4x) on standard multicore x86 CPUs for systems of {$<$} 200 stiff ODEs solved at low tolerance, a typical setup for a vast majority of users of high level language equation solver suites. The resulting method is distributed as the first widely available open source software for within-method parallel acceleration targeting typical modest compute architectures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Mathematical Software,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis},
  file = {/home/hermasl/Zotero/storage/K8PNH2LN/Utkarsh et al. - 2022 - Parallelizing Explicit and Implicit Extrapolation .pdf}
}

@misc{verdeStatisticalMethodsCosmology2009,
  title = {Statistical Methods in Cosmology},
  author = {Verde, Licia},
  year = 2009,
  month = nov,
  eprint = {0911.3105},
  primaryclass = {astro-ph},
  doi = {10.1007/978-3-642-10598-2_4},
  url = {http://arxiv.org/abs/0911.3105},
  urldate = {2025-04-30},
  abstract = {The advent of large data-set in cosmology has meant that in the past 10 or 20 years our knowledge and understanding of the Universe has changed not only quantitatively but also, and most importantly, qualitatively. Cosmologists are interested in studying the origin and evolution of the physical Universe. They rely on data where a host of useful information is enclosed, but is encoded in a non-trivial way. The challenges in extracting this information must be overcome to make the most of the large experimental effort. Even after having analyzed a decade or more of data and having converged to a standard cosmological model (the so-called and highly successful LCDM model) we should keep in mind that this model is described by 10 or more physical parameters and if we want to study deviations from the standard model the number of parameters is even larger. Dealing with such a high dimensional parameter space and finding parameters constraints is a challenge on itself. In addition, as gathering data is such an expensive and difficult process, cosmologists want to be able to compare and combine different data sets both for testing for possible disagreements (which could indicate new physics) and for improving parameter determinations. Finally, always because experiments are so expansive, cosmologists in many cases want to find out a priori, before actually doing the experiment, how much one would be able to learn from it. For all these reasons, more and more sophisiticated statistical techniques are being employed in cosmology, and it has become crucial to know some statistical background to understand recent literature in the field. Here, I will introduce some statistical tools that any cosmologist should know about in order to be able to understand recently published results from the analysis of cosmological data sets. I will not present a complete and rigorous introduction to statistics as there are several good books which are reported in the references. The reader should refer to those. I will take a practical approach and I will touch upon useful tools such as statistical inference, Bayesians vs Frequentist approach, chisquare and goodness of fit, confidence regions, likelihood, Fisher matrix approach, Monte Carlo methods and a brief introduction to model testing.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/S5XGXZFJ/Verde - 2009 - Statistical methods in cosmology.pdf}
}

@article{wolzValidityCosmologicalFisher2012,
  title = {On the {{Validity}} of {{Cosmological Fisher Matrix Forecasts}}},
  author = {Wolz, Laura and Kilbinger, Martin and Weller, Jochen and Giannantonio, Tommaso},
  year = 2012,
  month = sep,
  journal = {JCAP},
  volume = {2012},
  number = {09},
  eprint = {1205.3984},
  primaryclass = {astro-ph},
  pages = {009--009},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2012/09/009},
  url = {http://arxiv.org/abs/1205.3984},
  urldate = {2025-04-25},
  abstract = {We present a comparison of Fisher matrix forecasts for cosmological probes with Monte Carlo Markov Chain (MCMC) posterior likelihood estimation methods. We analyse the performance of future Dark Energy Task Force (DETF) stage-III and stageIV dark-energy surveys using supernovae, baryon acoustic oscillations and weak lensing as probes. We concentrate in particular on the dark-energy equation of state parameters w0 and wa. For purely geometrical probes, and especially when marginalising over wa, we find considerable disagreement between the two methods, since in this case the Fisher matrix can not reproduce the highly non-elliptical shape of the likelihood function. More quantitatively, the Fisher method underestimates the marginalized errors for purely geometrical probes between 30\%-70\%. For cases including structure formation such as weak lensing, we find that the posterior probability contours from the Fisher matrix estimation are in good agreement with the MCMC contours and the forecasted errors only changing on the 5\% level. We then explore non-linear transformations resulting in physically-motivated parameters and investigate whether these parameterisations exhibit a Gaussian behaviour. We conclude that for the purely geometrical probes and, more generally, in cases where it is not known whether the likelihood is close to Gaussian, the Fisher matrix is not the appropriate tool to produce reliable forecasts.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hermasl/Zotero/storage/F4HCAPX5/Wolz et al. - 2012 - On the Validity of Cosmological Fisher Matrix Fore.pdf}
}

@article{wongHowWellWe2008,
  title = {How Well Do We Understand Cosmological Recombination?},
  author = {Wong, Wan Yan and Moss, Adam and Scott, Douglas},
  year = 2008,
  month = may,
  journal = {MNRAS},
  volume = {386},
  number = {2},
  eprint = {0711.1357},
  primaryclass = {astro-ph},
  pages = {1023--1028},
  issn = {0035-8711, 1365-2966},
  doi = {10.1111/j.1365-2966.2008.13092.x},
  url = {http://arxiv.org/abs/0711.1357},
  urldate = {2025-09-16},
  abstract = {The major theoretical limitation for extracting cosmological parameters from the CMB sky lies in the precision with which we can calculate the cosmological recombination process. Uncertainty in the details of hydrogen and helium recombination could effectively increase the errors or bias the values of the cosmological parameters derived from the Planck satellite, for example. Here we modify the cosmological recombination code RECFAST by introducing one more parameter to reproduce the recent numerical results for the speed-up of the helium recombination. Together with the existing hydrogen fudge factor, we vary these two parameters to account for the remaining dominant uncertainties in cosmological recombination. By using the CosmoMC code with Planck forecast data, we find that we need to determine the parameters to better than ten per cent for He I and one per cent for H, in order to obtain negligible effects on the cosmological parameters. For helium recombination, if the existing studies have calculated the ionization fraction to the 0.1 per cent level by properly including the relevant physical processes, then we already have numerical calculations which are accurate enough for Planck. For hydrogen, setting the fudge factor to speed up low redshift recombination by 14 per cent appears to be sufficient for Planck. However, more work still needs to be done to carry out comprehensive numerical calculations of all the relevant effects for hydrogen, as well as to check for effects which couple hydrogen and helium recombinaton through the radiation field.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics},
  file = {/home/hermasl/Zotero/storage/ZNQR33IF/Wong et al. - 2008 - How well do we understand cosmological recombination.pdf;/home/hermasl/Zotero/storage/MSHA8WE5/0711.html}
}
